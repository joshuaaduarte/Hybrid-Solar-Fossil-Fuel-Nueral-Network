{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cd122d",
   "metadata": {},
   "source": [
    "# Project Two\n",
    "## Pavan M Reddy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678adb5",
   "metadata": {},
   "source": [
    "## Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "230f3090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909, 0.9558641975308642], [0.9900990099009901, 1.0, 1.0, 0.9969135802469136], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936, 0.9722222222222222], [1.0, 0.896551724137931, 1.009090909090909, 0.9540123456790124], [0.9900990099009901, 1.0, 1.0, 1.0030864197530864], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936, 0.9691358024691358], [1.188118811881188, 0.896551724137931, 1.009090909090909, 1.098456790123457], [1.7821782178217822, 1.0, 1.0, 1.4320987654320987]]\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.23 0.4 0.7 0.72 0.7\n",
      "-0.15 -0.12 0.01\n",
      "E3 =  0.0023304004322608684 icount = 8\n",
      "next ws: 1.2264341588876744 0.3957113223001962 0.6957952455947807 0.7187347373356996 0.6986778919002751\n",
      "next bs: -0.15420935585818193 -0.123030736217891 0.0078784846474763\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2264341588876744 0.3957113223001962 0.6957952455947807 0.7187347373356996 0.6986778919002751\n",
      "-0.15420935585818193 -0.123030736217891 0.0078784846474763\n",
      "E3 =  0.0011208050823701072 icount = 8\n",
      "next ws: 1.2237307002954811 0.39238966526058233 0.692525663164791 0.7177614507501813 0.6976596251353808\n",
      "next bs: -0.15748125247002986 -0.12538236196979707 0.006235455724596163\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2237307002954811 0.39238966526058233 0.692525663164791 0.7177614507501813 0.6976596251353808\n",
      "-0.15748125247002986 -0.12538236196979707 0.006235455724596163\n",
      "E3 =  0.0005694711971933216 icount = 8\n",
      "next ws: 1.2212760124942954 0.38922634213151786 0.6893831595751246 0.7168561263055452 0.6967132913148046\n",
      "next bs: -0.16062314487088403 -0.12763749121753512 0.004662143098987402\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2212760124942954 0.38922634213151786 0.6893831595751246 0.7168561263055452 0.6967132913148046\n",
      "-0.16062314487088403 -0.12763749121753512 0.004662143098987402\n",
      "E3 =  0.00035176283578148936 icount = 8\n",
      "next ws: 1.2155078523887517 0.3784189811906852 0.6775880837817743 0.7144048000682417 0.6941998145312959\n",
      "next bs: -0.17230251712890143 -0.13600992077209792 -0.0011710398522733981\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2155078523887517 0.3784189811906852 0.6775880837817743 0.7144048000682417 0.6941998145312959\n",
      "-0.17230251712890143 -0.13600992077209792 -0.0011710398522733981\n",
      "E3 =  0.0016797078268747107 icount = 8\n",
      "next ws: 1.2189392309467209 0.3822045801977516 0.6812466781789995 0.7156107060068335 0.6954841484028559\n",
      "next bs: -0.16863484339962156 -0.1333897170548162 0.0006479050822977917\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2189392309467209 0.3822045801977516 0.6812466781789995 0.7156107060068335 0.6954841484028559\n",
      "-0.16863484339962156 -0.1333897170548162 0.0006479050822977917\n",
      "E3 =  0.0008216333952222394 icount = 8\n",
      "next ws: 1.2218425714150614 0.3852945947715687 0.684217359081142 0.7166098045509699 0.6965480734788105\n",
      "next bs: -0.16565529842974683 -0.13125752277534503 0.0021308124049852425\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2218425714150614 0.3852945947715687 0.684217359081142 0.7166098045509699 0.6965480734788105\n",
      "-0.16565529842974683 -0.13125752277534503 0.0021308124049852425\n",
      "E3 =  0.00044417137698123886 icount = 8\n",
      "next ws: 1.2258282164520131 0.38905132425794764 0.6877709068984224 0.7179003895621714 0.6979302575789561\n",
      "next bs: -0.16208569321927208 -0.1286995086831426 0.0039125921928405034\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2258282164520131 0.38905132425794764 0.6877709068984224 0.7179003895621714 0.6979302575789561\n",
      "-0.16208569321927208 -0.1286995086831426 0.0039125921928405034\n",
      "E3 =  0.00039430050380493987 icount = 8\n",
      "next ws: 1.222086315201817 0.3832698487284558 0.681831771573015 0.7164149881793634 0.6963896171029743\n",
      "next bs: -0.1680040391981293 -0.13294829156692792 0.0009472380603631567\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.222086315201817 0.3832698487284558 0.681831771573015 0.7164149881793634 0.6963896171029743\n",
      "-0.1680040391981293 -0.13294829156692792 0.0009472380603631567\n",
      "E3 =  0.0006022902727817152 icount = 8\n",
      "next ws: 1.225149615306589 0.3863969185693395 0.6848230425535159 0.7174480624620733 0.697492103547914\n",
      "next bs: -0.16500243125907924 -0.13079789465075428 0.0024447521454367327\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.225149615306589 0.3863969185693395 0.6848230425535159 0.7174480624620733 0.697492103547914\n",
      "-0.16500243125907924 -0.13079789465075428 0.0024447521454367327\n",
      "E3 =  0.00037283618711337794 icount = 8\n",
      "next ws: 1.2436775756477836 0.3955471368447029 0.6930248357695544 0.7216212203699761 0.7021451631882852\n",
      "next bs: -0.1567231555895225 -0.12485794436304141 0.006587820566583621\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2436775756477836 0.3955471368447029 0.6930248357695544 0.7216212203699761 0.7021451631882852\n",
      "-0.1567231555895225 -0.12485794436304141 0.006587820566583621\n",
      "E3 =  0.002199294216083167 icount = 8\n",
      "next ws: 1.2402052077061227 0.39130090864283246 0.688860053754281 0.7203773193762014 0.7008415806319326\n",
      "next bs: -0.16089239071003572 -0.12786655289871554 0.004475340635333047\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2402052077061227 0.39130090864283246 0.688860053754281 0.7203773193762014 0.7008415806319326\n",
      "-0.16089239071003572 -0.12786655289871554 0.004475340635333047\n",
      "E3 =  0.001068342247457052 icount = 8\n",
      "next ws: 1.2375033631666228 0.38788616111745433 0.685495641928551 0.719391272086396 0.6998076567725898\n",
      "next bs: -0.16425894571504246 -0.13029174276875483 0.0027756667334821565\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2375033631666228 0.38788616111745433 0.685495641928551 0.719391272086396 0.6998076567725898\n",
      "-0.16425894571504246 -0.13029174276875483 0.0027756667334821565\n",
      "E3 =  0.0005591840629772685 icount = 8\n",
      "next ws: 1.2348043736383252 0.38417317002563217 0.6817917537794155 0.7183689384812942 0.6987388403405326\n",
      "next bs: -0.1679607705780251 -0.13295480326597695 0.0009120366070774876\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2348043736383252 0.38417317002563217 0.6817917537794155 0.7183689384812942 0.6987388403405326\n",
      "-0.1679607705780251 -0.13295480326597695 0.0009120366070774876\n",
      "E3 =  0.0003991424095178773 icount = 8\n",
      "next ws: 1.1830706842822987 0.40277043178621363 0.6973441941364044 0.742321916118297 0.7369833982752259\n",
      "next bs: -0.15216976822198802 -0.12161103766591498 0.008838366227559609\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1830706842822987 0.40277043178621363 0.6973441941364044 0.742321916118297 0.7369833982752259\n",
      "-0.15216976822198802 -0.12161103766591498 0.008838366227559609\n",
      "E3 =  0.012064893122949543 icount = 8\n",
      "next ws: 1.175958793077181 0.3943435536005123 0.6891112148026566 0.7396744731962283 0.7341545274570819\n",
      "next bs: -0.16041465032533794 -0.12773139434704314 0.004327764962045294\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.175958793077181 0.3943435536005123 0.6891112148026566 0.7396744731962283 0.7341545274570819\n",
      "-0.16041465032533794 -0.12773139434704314 0.004327764962045294\n",
      "E3 =  0.0056459258537032994 icount = 8\n",
      "next ws: 1.17099864835171 0.3884046351825693 0.683300317457445 0.7377995113482512 0.7321421650835188\n",
      "next bs: -0.16623313574449325 -0.13203517948425678 0.0011681216183574042\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.17099864835171 0.3884046351825693 0.683300317457445 0.7377995113482512 0.7321421650835188\n",
      "-0.16623313574449325 -0.13203517948425678 0.0011681216183574042\n",
      "E3 =  0.0026633665855129727 icount = 8\n",
      "next ws: 1.167461058938615 0.38409875322812126 0.6790772183385893 0.7364425129895482 0.7306818802100499\n",
      "next bs: -0.17046080228550978 -0.13515434979236213 -0.001115554484283074\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.167461058938615 0.38409875322812126 0.6790772183385893 0.7364425129895482 0.7306818802100499\n",
      "-0.17046080228550978 -0.13515434979236213 -0.001115554484283074\n",
      "E3 =  0.001283238587839526 icount = 8\n",
      "next ws: 1.164777052129941 0.3807370901090618 0.6757661798942247 0.7353947561742071 0.7295533885823193\n",
      "next bs: -0.1737740952126002 -0.1375943995618591 -0.0028984546376652244\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.164777052129941 0.3807370901090618 0.6757661798942247 0.7353947561742071 0.7295533885823193\n",
      "-0.1737740952126002 -0.1375943995618591 -0.0028984546376652244\n",
      "E3 =  0.000655254128778413 icount = 8\n",
      "next ws: 1.1623227006167953 0.3774563681322022 0.6725018589606904 0.7344068474244198 0.7284915428538334\n",
      "next bs: -0.17703748561440674 -0.13999427975069687 -0.004649295361623389\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1623227006167953 0.3774563681322022 0.6725018589606904 0.7344068474244198 0.7284915428538334\n",
      "-0.17703748561440674 -0.13999427975069687 -0.004649295361623389\n",
      "E3 =  0.00041378582851962943 icount = 8\n",
      "next ws: 1.155985870259448 0.3614102647982093 0.6538385595896244 0.7312594578674627 0.7252241343416739\n",
      "next bs: -0.19539935388745774 -0.15347936154193073 -0.014473063401229487\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.155985870259448 0.3614102647982093 0.6538385595896244 0.7312594578674627 0.7252241343416739\n",
      "-0.19539935388745774 -0.15347936154193073 -0.014473063401229487\n",
      "E3 =  0.003872292257304887 icount = 8\n",
      "next ws: 1.1605712263572872 0.36648530129985607 0.6587548335328186 0.7330145288522137 0.7271629310095135\n",
      "next bs: -0.19047208229845083 -0.14987624759098778 -0.011859998205222474\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1605712263572872 0.36648530129985607 0.6587548335328186 0.7330145288522137 0.7271629310095135\n",
      "-0.19047208229845083 -0.14987624759098778 -0.011859998205222474\n",
      "E3 =  0.0018380946987153924 icount = 8\n",
      "next ws: 1.1640486624161233 0.37022809632953346 0.6623693908361727 0.7343203399435307 0.728603077107352\n",
      "next bs: -0.1868484143678162 -0.14722004635009675 -0.009928507125545057\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1640486624161233 0.37022809632953346 0.6623693908361727 0.7343203399435307 0.728603077107352\n",
      "-0.1868484143678162 -0.14722004635009675 -0.009928507125545057\n",
      "E3 =  0.0009101291070356737 icount = 8\n",
      "next ws: 1.1671769918035626 0.3734037514052771 0.6654174682089989 0.7354590370884613 0.7298607949620566\n",
      "next bs: -0.18379093841875982 -0.14497487957181648 -0.008292671702270858\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1671769918035626 0.3734037514052771 0.6654174682089989 0.7354590370884613 0.7298607949620566\n",
      "-0.18379093841875982 -0.14497487957181648 -0.008292671702270858\n",
      "E3 =  0.0005137693796898051 icount = 8\n",
      "next ws: 1.1732783908661468 0.37812679193722737 0.6698443952638018 0.7373976820964104 0.7320390189022836\n",
      "next bs: -0.17934084365987 -0.14170201716549097 -0.00590393774458868\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1732783908661468 0.37812679193722737 0.6698443952638018 0.7373976820964104 0.7320390189022836\n",
      "-0.17934084365987 -0.14170201716549097 -0.00590393774458868\n",
      "E3 =  0.0006672260752738724 icount = 8\n",
      "next ws: 1.170623771143426 0.37440279314749714 0.6661279788035627 0.7363060030050412 0.7308653837764308\n",
      "next bs: -0.18305527891542384 -0.14444103311323356 -0.007909004291731879\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.170623771143426 0.37440279314749714 0.6661279788035627 0.7363060030050412 0.7308653837764308\n",
      "-0.18305527891542384 -0.14444103311323356 -0.007909004291731879\n",
      "E3 =  0.0004667491637744217 icount = 8\n",
      "next ws: 1.1498686665623672 0.3999735756224973 0.6863063486166414 0.6945787805099609 0.7039764821493603\n",
      "next bs: -0.1624821206006896 -0.12929289314532164 0.0031622468394151525\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1498686665623672 0.3999735756224973 0.6863063486166414 0.6945787805099609 0.7039764821493603\n",
      "-0.1624821206006896 -0.12929289314532164 0.0031622468394151525\n",
      "E3 =  0.00403742445952513 icount = 8\n",
      "next ws: 1.1546358806731771 0.405531131846702 0.6916907470037065 0.6962707020028446 0.7058485935096488\n",
      "next bs: -0.1570855210968272 -0.12554452964302823 0.005801006591576717\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1546358806731771 0.405531131846702 0.6916907470037065 0.6962707020028446 0.7058485935096488\n",
      "-0.1570855210968272 -0.12554452964302823 0.005801006591576717\n",
      "E3 =  0.0018923786517418835 icount = 8\n",
      "next ws: 1.1580286943996592 0.40949680179073666 0.6955219432941673 0.6974665855456641 0.7071672492020082\n",
      "next bs: -0.1532445557113491 -0.12287017797771277 0.007688693953089819\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1580286943996592 0.40949680179073666 0.6955219432941673 0.6974665855456641 0.7071672492020082\n",
      "-0.1532445557113491 -0.12287017797771277 0.007688693953089819\n",
      "E3 =  0.0009071131971644992 icount = 8\n",
      "next ws: 1.1606263892893292 0.41254721074540435 0.6984542362276424 0.6983781567341151 0.7081699959108821\n",
      "next bs: -0.15030332927739684 -0.12081877081950744 0.009139381910151193\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1606263892893292 0.41254721074540435 0.6984542362276424 0.6983781567341151 0.7081699959108821\n",
      "-0.15030332927739684 -0.12081877081950744 0.009139381910151193\n",
      "E3 =  0.000460595137093609 icount = 8\n",
      "next ws: 1.1630778885373607 0.41545711694896326 0.7012193472321739 0.699236459210893 0.7091125013528717\n",
      "next bs: -0.1475266383815076 -0.11887959054981592 0.010512651193809102\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1630778885373607 0.41545711694896326 0.7012193472321739 0.699236459210893 0.7091125013528717\n",
      "-0.1475266383815076 -0.11887959054981592 0.010512651193809102\n",
      "E3 =  0.00028667222349910103 icount = 8\n",
      "next ws: 1.1714705245646795 0.42659672234586804 0.7106162005062585 0.7022114366360884 0.7123769875766794\n",
      "next bs: -0.13798550767907175 -0.11220808410057635 0.015243499819821189\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.1630778885373607 0.41545711694896326 0.7012193472321739 0.699236459210893 0.7091125013528717\n",
      "-0.1475266383815076 -0.11887959054981592 0.010512651193809102\n",
      "Tdbin, Twbin, qdot, Tdbout, ypredicted:\n",
      "20.0 13.0 310.8 30.97 31.090793913544566\n",
      "20.0 14.5 308.0 32.3 31.678835486931895\n",
      "20.0 15.3 306.0 31.5 31.973926195676942\n",
      "20.2 13.0 310.8 30.91 31.27579387700364\n",
      "20.0 14.5 308.0 32.5 31.678835486931895\n",
      "20.0 15.3 306.0 31.4 31.973926195676942\n",
      "24.0 13.0 310.8 35.59 34.79079318272606\n",
      "36.0 14.5 308.0 46.4 46.47883256365786\n"
     ]
    }
   ],
   "source": [
    "'''#Intro to Neural Network Modeling \n",
    "# Python Neural Network Model of Spray Cooling Test System\n",
    "\n",
    ">>>>> start CodeP2.1F22\n",
    "    V.P. Carey, ME249, Fall 2022'''\n",
    "\n",
    "# version 3 print function\n",
    "from __future__ import print_function\n",
    "\n",
    "# import math, numpy and other usefuk packages\n",
    "import math\n",
    "import numpy \n",
    "\n",
    "%matplotlib inline\n",
    "# importing the required module \n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = [10, 8] # for square canvas\n",
    "\n",
    "\n",
    "#assembling data array\n",
    "#store array where rows are data vectors [x01, x02, x03, y3]\n",
    "xydata = []\n",
    "\n",
    "xydata = [[20./20.2, 13.0/14.5, 310.8/308.0, 30.97/32.4], [20./20.2, 14.5/14.5, 308.0/308.0, 32.3/32.4]]\n",
    "xydata.append([20./20.2, 15.3/14.5, 306.0/308.0, 31.5/32.4])\n",
    "xydata.append([20.2/20.2, 13.0/14.5, 310.8/308.0, 30.91/32.4]) \n",
    "xydata.append([20./20.2, 14.5/14.5, 308.0/308.0, 32.5/32.4]) \n",
    "xydata.append([20./20.2, 15.3/14.5, 306.0/308.0, 31.4/32.4]) \n",
    "xydata.append([24./20.2, 13.0/14.5, 310.8/308.0, 35.59/32.4]) \n",
    "xydata.append([36./20.2, 14.5/14.5, 308.0/308.0, 46.4/32.4]) \n",
    "print (xydata)\n",
    "\n",
    "#set starting values \n",
    "w01n =  1.23 \n",
    "w02n =  0.40 \n",
    "w03n =  0.70\n",
    "b1n =  -0.15\n",
    "w12n =  0.72\n",
    "b2n =  -0.12\n",
    "w23n =  0.7\n",
    "b3n =  0.01\n",
    "\n",
    "#start of batch loop  \n",
    "\n",
    "for k in range (0,200):\n",
    "    icount = 0\n",
    "    #initialize error and derivative parameters\n",
    "    E3ti = 0.\n",
    "    dE3da3 = 0.\n",
    "    dE3dw01ti = 0.\n",
    "    dE3dw02ti = 0.\n",
    "    dE3dw03ti = 0.\n",
    "    dE3db1ti = 0.\n",
    "    dE3dw12ti = 0.\n",
    "    dE3db2ti = 0.\n",
    "    dE3dw23ti = 0.\n",
    "    dE3db3ti = 0.\n",
    " \n",
    "    w01 = w01n \n",
    "    w02 = w02n\n",
    "    w03 = w03n\n",
    "    b1 = b1n \n",
    "    w12 = w12n\n",
    "    b2 = b2n \n",
    "    w23 = w23n \n",
    "    b3 = b3n \n",
    "    \n",
    "    #doing calcuations for each data point \n",
    "    for i in range(0,8):\n",
    "        #compute activation functions and their derivatives\n",
    "        z1 = w01*xydata[i][0]+w02*xydata[i][1]+w03*xydata[i][2]+b1 \n",
    "        sig1 = z1\n",
    "        sigp1 = 1.0\n",
    "        if z1 < 0.0:\n",
    "            sig1 = math.exp(z1) - 1.0\n",
    "            sigp1 = math.exp(z1)\n",
    "        a1 = sig1\n",
    "\n",
    "        z2 = w12*a1+b2 \n",
    "        sig2 = z2\n",
    "        sigp2 = 1.0\n",
    "        if z2 < 0.0:\n",
    "            sig2 = math.exp(z2) - 1.0\n",
    "            sigp2 = math.exp(z2)\n",
    "        a2 = sig2\n",
    "\n",
    "        z3 = w23*a2+b3 \n",
    "        sig3 = z3\n",
    "        sigp3 = 1.0\n",
    "        if z3 < 0.0:\n",
    "            sig3 = math.exp(z3) - 1.0\n",
    "            sigp3 = math.exp(z3)\n",
    "        a3 = sig3\n",
    "        \n",
    "        \n",
    "        #compute derivatives for backpropagation\n",
    "        #add to sum for batch average calculation\n",
    "        E3ti = E3ti +(a3 - xydata[i][3])*(a3 - xydata[i][3])\n",
    "        dE3da3 = 2.*(a3 - xydata[i][3])\n",
    "        \n",
    "        dE3dw01ti = dE3dw01ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][0]\n",
    "        dE3dw02ti = dE3dw02ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][1]\n",
    "        dE3dw03ti = dE3dw03ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][2]\n",
    "        dE3db1ti = dE3db1ti + dE3da3*sigp3*w23*sigp2*w12*sigp1\n",
    "        \n",
    "        dE3dw12ti = dE3dw12ti + dE3da3*sigp3*w23*sigp2*a1\n",
    "        dE3db2ti = dE3db2ti + dE3da3*sigp3*w23*sigp2\n",
    "        \n",
    "        dE3dw23ti = dE3dw23ti + dE3da3*sigp3*a2\n",
    "        dE3db3ti = dE3db3ti + dE3da3*sigp3\n",
    "        \n",
    "        icount = i + 1\n",
    "        # end  calculations for each data point in batch\n",
    "        \n",
    "    #compute batch averaged values\n",
    "    E3 = E3ti/icount\n",
    "    dE3dw01 = dE3dw01ti/icount\n",
    "    dE3dw02 = dE3dw02ti/icount\n",
    "    dE3dw03 = dE3dw03ti/icount\n",
    "    dE3db1 = dE3db1ti/icount\n",
    "    dE3dw12 = dE3dw12ti/icount\n",
    "    dE3db2 = dE3db2ti/icount\n",
    "    dE3dw23 = dE3dw23ti/icount\n",
    "    dE3db3 = dE3db3ti/icount\n",
    "    \n",
    "    #set gam = learning rate\n",
    "    gam = 0.165\n",
    "    if E3 < 0.07: \n",
    "        gam = 0.08\n",
    "\n",
    "    w01n = w01 + gam*(-E3)/dE3dw01\n",
    "    w02n = w02 + gam*(-E3)/dE3dw02\n",
    "    w03n = w03 + gam*(-E3)/dE3dw03\n",
    "    b1n = b1 + gam*(-E3)/dE3db1\n",
    "    w12n = w12 + gam*(-E3)/dE3dw12\n",
    "    b2n = b2 + gam*(-E3)/dE3db2\n",
    "    \n",
    "    w23n = w23 + gam*(-E3)/dE3dw23\n",
    "    b3n = b3 + gam*(-E3)/dE3db3\n",
    "    \n",
    "    #printing for each iteration\n",
    "    print ('last w01, w02, w03, w12, w23:')\n",
    "    print ('last b1, b2, b3:')\n",
    "    print (w01, w02, w03, w12, w23)\n",
    "    print (b1, b2, b3)\n",
    "    print ('E3 = ', E3, 'icount =', icount)\n",
    "    print ('next ws:', w01n, w02n, w03n, w12n, w23n)\n",
    "    print ('next bs:', b1n, b2n, b3n)\n",
    "    \n",
    "    #quit if squared error is below target\n",
    "    if E3 < 0.00035:\n",
    "        break\n",
    "    \n",
    "\n",
    "print ('last w01, w02, w03, w12, w23:')\n",
    "print ('last b1, b2, b3:')\n",
    "print (w01, w02, w03, w12, w23)\n",
    "print (b1, b2, b3)\n",
    "#decomment print statements below if you want to print neuron outputs\n",
    "#print ('z1 =', z1)\n",
    "#print ('a1 =', a1)\n",
    "#print ('z2 =', z2)\n",
    "#print ('a2 =', a2)\n",
    "#print ('z3 =', z3)\n",
    "#print ('a3 =', a3)\n",
    "\n",
    "#print comparison of data and trained network predictions\n",
    "# restore raw data values  \n",
    "xydatar = [[20., 13.0, 310.8, 30.97], [20., 14.5, 308.0, 32.3]]\n",
    "xydatar.append([20., 15.3, 306.0, 31.5])\n",
    "xydatar.append([20.2, 13.0, 310.8, 30.91]) \n",
    "xydatar.append([20., 14.5, 308.0, 32.5]) \n",
    "xydatar.append([20., 15.3, 306.0, 31.4]) \n",
    "xydatar.append([24., 13.0, 310.8, 35.59]) \n",
    "xydatar.append([36., 14.5, 308.0, 46.4])\n",
    "print ('Tdbin, Twbin, qdot, Tdbout, ypredicted:')\n",
    "for i in range(0,8): \n",
    "    z1 = w01*xydata[i][0]+w02*xydata[i][1]+w03*xydata[i][2]+b1 \n",
    "    sig1 = z1\n",
    "    sigp1 = 1.0\n",
    "    if z1 < 0.0:\n",
    "        sig1 = math.exp(z1) - 1.0\n",
    "        sigp1 = math.exp(z1)\n",
    "    a1 = sig1\n",
    "\n",
    "    z2 = w12*a1+b2 \n",
    "    sig2 = z2\n",
    "    sigp2 = 1.0\n",
    "    if z2 < 0.0:\n",
    "        sig2 = math.exp(z2) - 1.0\n",
    "        sigp2 = math.exp(z2)\n",
    "    a2 = sig2\n",
    "\n",
    "    z3 = w23*a2+b3 \n",
    "    sig3 = z3\n",
    "    sigp3 = 1.0\n",
    "    if z3 < 0.0:\n",
    "        sig3 = math.exp(z3) - 1.0\n",
    "        sigp3 = math.exp(z3)\n",
    "    a3 = sig3\n",
    "\n",
    "    print (xydatar[i][0], xydatar[i][1], xydatar[i][2], xydatar[i][3], a3*32.4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e738a",
   "metadata": {},
   "source": [
    "## Task 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892719d",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ece8f3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000000\n",
      "Name: x03, dtype: float64 0    0.955835\n",
      "1    0.996883\n",
      "2    0.972192\n",
      "3    0.953983\n",
      "4    1.003055\n",
      "5    0.969106\n",
      "6    1.098423\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
    "    'y3' : [30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "\n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2016a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "Weights and biases of the layers before training the model: \n",
      "\n",
      "dense_one\n",
      "Weights\n",
      "Shape:  (3, 1) \n",
      " [[1.23]\n",
      " [0.4 ]\n",
      " [0.7 ]]\n",
      "Bias\n",
      "Shape:  (1,) \n",
      " [-0.15] \n",
      "\n",
      "dense_two\n",
      "Weights\n",
      "Shape:  (1, 1) \n",
      " [[0.72]]\n",
      "Bias\n",
      "Shape:  (1,) \n",
      " [-0.12] \n",
      "\n",
      "dense_three\n",
      "Weights\n",
      "Shape:  (1, 1) \n",
      " [[0.7]]\n",
      "Bias\n",
      "Shape:  (1,) \n",
      " [0.01] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Dense at 0x7ff4ca578810>,\n",
       " <keras.layers.core.Dense at 0x7ff4ca578e90>,\n",
       " <keras.layers.core.Dense at 0x7ff4ca578f50>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "import keras\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3],  kernel_initializer=initializer, name=\"dense_one\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_two\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_three\")\n",
    "  ])\n",
    "\n",
    "\n",
    "#set starting values to those used in first principles model\n",
    "w01n =  1.23 \n",
    "w02n =  0.40 \n",
    "w03n =  0.70\n",
    "b1n =  -0.15\n",
    "w12n =  0.72\n",
    "b2n =  -0.12\n",
    "w23n =  0.7\n",
    "b3n =  0.01\n",
    "\n",
    "weights0 =  [[ w01n], [w02n], [ w03n]]\n",
    "w0array= np.array(weights0)\n",
    "print(np.shape(w0array))\n",
    "bias0 = [b1n]\n",
    "bias0array= np.array(bias0)\n",
    "L0=[]\n",
    "L0.append(w0array)\n",
    "L0.append(bias0array)\n",
    "model.layers[0].set_weights(L0) \n",
    "\n",
    "weights1 =  [[ w12n]]\n",
    "w1array= np.array(weights1)\n",
    "print(np.shape(w1array))\n",
    "bias1 = [b2n]\n",
    "bias1array= np.array(bias1)\n",
    "L1=[]\n",
    "L1.append(w1array)\n",
    "L1.append(bias1array)\n",
    "model.layers[1].set_weights(L1)\n",
    "\n",
    "weights2 =  [[ w23n]]\n",
    "w2array= np.array(weights2)\n",
    "print(np.shape(w2array))\n",
    "bias2 = [b3n]\n",
    "bias2array= np.array(bias2)\n",
    "L2=[]\n",
    "L2.append(w2array)\n",
    "L2.append(bias2array)\n",
    "model.layers[2].set_weights(L2)\n",
    "\n",
    "\n",
    "print(\"Weights and biases of the layers before training the model: \\n\")\n",
    "for layer in model.layers:\n",
    "  print(layer.name)\n",
    "  print(\"Weights\")\n",
    "  print(\"Shape: \",layer.get_weights()[0].shape,'\\n',layer.get_weights()[0])\n",
    "  print(\"Bias\")\n",
    "  print(\"Shape: \",layer.get_weights()[1].shape,'\\n',layer.get_weights()[1],'\\n')\n",
    "model.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c487a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "rms = keras.optimizers.RMSprop(0.00035)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4691dfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0150\n",
      "Epoch 2/400\n",
      "8/8 [==============================] - 0s 172us/step - loss: 0.0148\n",
      "Epoch 3/400\n",
      "8/8 [==============================] - 0s 178us/step - loss: 0.0148\n",
      "Epoch 4/400\n",
      "8/8 [==============================] - 0s 148us/step - loss: 0.0140\n",
      "Epoch 5/400\n",
      "8/8 [==============================] - 0s 165us/step - loss: 0.0146\n",
      "Epoch 6/400\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0144\n",
      "Epoch 7/400\n",
      "8/8 [==============================] - 0s 208us/step - loss: 0.0138\n",
      "Epoch 8/400\n",
      "8/8 [==============================] - 0s 184us/step - loss: 0.0147\n",
      "Epoch 9/400\n",
      "8/8 [==============================] - 0s 194us/step - loss: 0.0142\n",
      "Epoch 10/400\n",
      "8/8 [==============================] - 0s 207us/step - loss: 0.0138\n",
      "Epoch 11/400\n",
      "8/8 [==============================] - 0s 181us/step - loss: 0.0142\n",
      "Epoch 12/400\n",
      "8/8 [==============================] - 0s 176us/step - loss: 0.0138\n",
      "Epoch 13/400\n",
      "8/8 [==============================] - 0s 224us/step - loss: 0.0143\n",
      "Epoch 14/400\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0137\n",
      "Epoch 15/400\n",
      "8/8 [==============================] - 0s 196us/step - loss: 0.0143\n",
      "Epoch 16/400\n",
      "8/8 [==============================] - 0s 181us/step - loss: 0.0137\n",
      "Epoch 17/400\n",
      "8/8 [==============================] - 0s 173us/step - loss: 0.0138\n",
      "Epoch 18/400\n",
      "8/8 [==============================] - 0s 187us/step - loss: 0.0143\n",
      "Epoch 19/400\n",
      "8/8 [==============================] - 0s 204us/step - loss: 0.0138\n",
      "Epoch 20/400\n",
      "8/8 [==============================] - 0s 212us/step - loss: 0.0143\n",
      "Epoch 21/400\n",
      "8/8 [==============================] - 0s 181us/step - loss: 0.0137\n",
      "Epoch 22/400\n",
      "8/8 [==============================] - 0s 188us/step - loss: 0.0138\n",
      "Epoch 23/400\n",
      "8/8 [==============================] - 0s 182us/step - loss: 0.0143\n",
      "Epoch 24/400\n",
      "8/8 [==============================] - 0s 185us/step - loss: 0.0138\n",
      "Epoch 25/400\n",
      "8/8 [==============================] - 0s 163us/step - loss: 0.0143\n",
      "Epoch 26/400\n",
      "8/8 [==============================] - 0s 166us/step - loss: 0.0137\n",
      "Epoch 27/400\n",
      "8/8 [==============================] - 0s 195us/step - loss: 0.0143\n",
      "Epoch 28/400\n",
      "8/8 [==============================] - 0s 199us/step - loss: 0.0137\n",
      "Epoch 29/400\n",
      "8/8 [==============================] - 0s 187us/step - loss: 0.0138\n",
      "Epoch 30/400\n",
      "8/8 [==============================] - 0s 230us/step - loss: 0.0143\n",
      "Epoch 31/400\n",
      "8/8 [==============================] - 0s 212us/step - loss: 0.0137\n",
      "Epoch 32/400\n",
      "8/8 [==============================] - 0s 193us/step - loss: 0.0143\n",
      "Epoch 33/400\n",
      "8/8 [==============================] - 0s 212us/step - loss: 0.0137\n",
      "Epoch 34/400\n",
      "8/8 [==============================] - 0s 225us/step - loss: 0.0138\n",
      "Epoch 35/400\n",
      "8/8 [==============================] - 0s 214us/step - loss: 0.0143\n",
      "Epoch 36/400\n",
      "8/8 [==============================] - 0s 190us/step - loss: 0.0138\n",
      "Epoch 37/400\n",
      "8/8 [==============================] - 0s 165us/step - loss: 0.0143\n",
      "Epoch 38/400\n",
      "8/8 [==============================] - 0s 199us/step - loss: 0.0137\n",
      "Epoch 39/400\n",
      "8/8 [==============================] - 0s 186us/step - loss: 0.0138\n",
      "Epoch 40/400\n",
      "8/8 [==============================] - 0s 197us/step - loss: 0.0143\n",
      "Epoch 41/400\n",
      "8/8 [==============================] - 0s 164us/step - loss: 0.0138\n",
      "Epoch 42/400\n",
      "8/8 [==============================] - 0s 177us/step - loss: 0.0143\n",
      "Epoch 43/400\n",
      "8/8 [==============================] - 0s 218us/step - loss: 0.0137\n",
      "Epoch 44/400\n",
      "8/8 [==============================] - 0s 206us/step - loss: 0.0138\n",
      "Epoch 45/400\n",
      "8/8 [==============================] - 0s 183us/step - loss: 0.0143\n",
      "Epoch 46/400\n",
      "8/8 [==============================] - 0s 201us/step - loss: 0.0138\n",
      "Epoch 47/400\n",
      "8/8 [==============================] - 0s 223us/step - loss: 0.0143\n",
      "Epoch 48/400\n",
      "8/8 [==============================] - 0s 212us/step - loss: 0.0137\n",
      "Epoch 49/400\n",
      "8/8 [==============================] - 0s 203us/step - loss: 0.0138\n",
      "Epoch 50/400\n",
      "8/8 [==============================] - 0s 191us/step - loss: 0.0143\n",
      "Epoch 51/400\n",
      "8/8 [==============================] - 0s 207us/step - loss: 0.0138\n",
      "Epoch 52/400\n",
      "8/8 [==============================] - 0s 158us/step - loss: 0.0143\n",
      "Epoch 53/400\n",
      "8/8 [==============================] - 0s 188us/step - loss: 0.0137\n",
      "Epoch 54/400\n",
      "8/8 [==============================] - 0s 198us/step - loss: 0.0138\n",
      "Epoch 55/400\n",
      "8/8 [==============================] - 0s 206us/step - loss: 0.0143\n",
      "Epoch 56/400\n",
      "8/8 [==============================] - 0s 190us/step - loss: 0.0138\n",
      "Epoch 57/400\n",
      "8/8 [==============================] - 0s 198us/step - loss: 0.0143\n",
      "Epoch 58/400\n",
      "8/8 [==============================] - 0s 165us/step - loss: 0.0137\n",
      "Epoch 59/400\n",
      "8/8 [==============================] - 0s 170us/step - loss: 0.0144\n",
      "Epoch 60/400\n",
      "8/8 [==============================] - 0s 189us/step - loss: 0.0137\n",
      "Epoch 61/400\n",
      "8/8 [==============================] - 0s 198us/step - loss: 0.0138\n",
      "Epoch 62/400\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0143\n",
      "Epoch 63/400\n",
      "8/8 [==============================] - 0s 201us/step - loss: 0.0137\n",
      "Epoch 64/400\n",
      "8/8 [==============================] - 0s 221us/step - loss: 0.0143\n",
      "Epoch 65/400\n",
      "8/8 [==============================] - 0s 202us/step - loss: 0.0137\n",
      "Epoch 66/400\n",
      "8/8 [==============================] - 0s 229us/step - loss: 0.0138\n",
      "Epoch 67/400\n",
      "8/8 [==============================] - 0s 171us/step - loss: 0.0143\n",
      "Epoch 68/400\n",
      "8/8 [==============================] - 0s 178us/step - loss: 0.0137\n",
      "Epoch 69/400\n",
      "8/8 [==============================] - 0s 171us/step - loss: 0.0143\n",
      "Epoch 70/400\n",
      "8/8 [==============================] - 0s 160us/step - loss: 0.0137\n",
      "Epoch 71/400\n",
      "8/8 [==============================] - 0s 201us/step - loss: 0.0138\n",
      "Epoch 72/400\n",
      "8/8 [==============================] - 0s 212us/step - loss: 0.0143\n",
      "Epoch 73/400\n",
      "8/8 [==============================] - 0s 194us/step - loss: 0.0137\n",
      "Epoch 74/400\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0143\n",
      "Epoch 75/400\n",
      "8/8 [==============================] - 0s 165us/step - loss: 0.0137\n",
      "Epoch 76/400\n",
      "8/8 [==============================] - 0s 185us/step - loss: 0.0138\n",
      "Epoch 77/400\n",
      "8/8 [==============================] - 0s 178us/step - loss: 0.0143\n",
      "Epoch 78/400\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0138\n",
      "Epoch 79/400\n",
      "8/8 [==============================] - 0s 183us/step - loss: 0.0143\n",
      "Epoch 80/400\n",
      "8/8 [==============================] - 0s 212us/step - loss: 0.0137\n",
      "Epoch 81/400\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0138\n",
      "Epoch 82/400\n",
      "8/8 [==============================] - 0s 199us/step - loss: 0.0143\n",
      "Epoch 83/400\n",
      "8/8 [==============================] - 0s 199us/step - loss: 0.0138\n",
      "Epoch 84/400\n",
      "8/8 [==============================] - 0s 195us/step - loss: 0.0143\n",
      "Epoch 85/400\n",
      "8/8 [==============================] - 0s 188us/step - loss: 0.0137\n",
      "Epoch 86/400\n",
      "8/8 [==============================] - 0s 195us/step - loss: 0.0138\n",
      "Epoch 87/400\n",
      "8/8 [==============================] - 0s 164us/step - loss: 0.0143\n",
      "Epoch 88/400\n",
      "8/8 [==============================] - 0s 176us/step - loss: 0.0138\n",
      "Epoch 89/400\n",
      "8/8 [==============================] - 0s 182us/step - loss: 0.0143\n",
      "Epoch 90/400\n",
      "8/8 [==============================] - 0s 193us/step - loss: 0.0137\n",
      "Epoch 91/400\n",
      "8/8 [==============================] - 0s 223us/step - loss: 0.0138\n",
      "Epoch 92/400\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0143\n",
      "Epoch 93/400\n",
      "8/8 [==============================] - 0s 271us/step - loss: 0.0138\n",
      "Epoch 94/400\n",
      "8/8 [==============================] - 0s 224us/step - loss: 0.0143\n",
      "Epoch 95/400\n",
      "8/8 [==============================] - 0s 278us/step - loss: 0.0137\n",
      "Epoch 96/400\n",
      "8/8 [==============================] - 0s 200us/step - loss: 0.0138\n",
      "Epoch 97/400\n",
      "8/8 [==============================] - 0s 222us/step - loss: 0.0143\n",
      "Epoch 98/400\n",
      "8/8 [==============================] - 0s 232us/step - loss: 0.0138\n",
      "Epoch 99/400\n",
      "8/8 [==============================] - 0s 194us/step - loss: 0.0143\n",
      "Epoch 100/400\n",
      "8/8 [==============================] - 0s 206us/step - loss: 0.0137\n",
      "Epoch 101/400\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0138\n",
      "Epoch 102/400\n",
      "8/8 [==============================] - 0s 157us/step - loss: 0.0143\n",
      "Epoch 103/400\n",
      "8/8 [==============================] - 0s 160us/step - loss: 0.0138\n",
      "Epoch 104/400\n",
      "8/8 [==============================] - 0s 180us/step - loss: 0.0143\n",
      "Epoch 105/400\n",
      "8/8 [==============================] - 0s 158us/step - loss: 0.0137\n",
      "Epoch 106/400\n",
      "8/8 [==============================] - 0s 173us/step - loss: 0.0138\n",
      "Epoch 107/400\n",
      "8/8 [==============================] - 0s 177us/step - loss: 0.0143\n",
      "Epoch 108/400\n",
      "8/8 [==============================] - 0s 189us/step - loss: 0.0138\n",
      "Epoch 109/400\n",
      "8/8 [==============================] - 0s 179us/step - loss: 0.0143\n",
      "Epoch 110/400\n",
      "8/8 [==============================] - 0s 241us/step - loss: 0.0137\n",
      "Epoch 111/400\n",
      "8/8 [==============================] - 0s 207us/step - loss: 0.0143\n",
      "Epoch 112/400\n",
      "8/8 [==============================] - 0s 229us/step - loss: 0.0137\n",
      "Epoch 113/400\n",
      "8/8 [==============================] - 0s 185us/step - loss: 0.0138\n",
      "Epoch 114/400\n",
      "8/8 [==============================] - 0s 210us/step - loss: 0.0143\n",
      "Epoch 115/400\n",
      "8/8 [==============================] - 0s 309us/step - loss: 0.0137\n",
      "Epoch 116/400\n",
      "8/8 [==============================] - 0s 279us/step - loss: 0.0143\n",
      "Epoch 117/400\n",
      "8/8 [==============================] - 0s 173us/step - loss: 0.0137\n",
      "Epoch 118/400\n",
      "8/8 [==============================] - 0s 190us/step - loss: 0.0138\n",
      "Epoch 119/400\n",
      "8/8 [==============================] - 0s 194us/step - loss: 0.0143\n",
      "Epoch 120/400\n",
      "8/8 [==============================] - 0s 252us/step - loss: 0.0137\n",
      "Epoch 121/400\n",
      "8/8 [==============================] - 0s 195us/step - loss: 0.0143\n",
      "Epoch 122/400\n",
      "8/8 [==============================] - 0s 251us/step - loss: 0.0137\n",
      "Epoch 123/400\n",
      "8/8 [==============================] - 0s 187us/step - loss: 0.0138\n",
      "Epoch 124/400\n",
      "8/8 [==============================] - 0s 232us/step - loss: 0.0143\n",
      "Epoch 125/400\n",
      "8/8 [==============================] - 0s 221us/step - loss: 0.0137\n",
      "Epoch 126/400\n",
      "8/8 [==============================] - 0s 179us/step - loss: 0.0143\n",
      "Epoch 127/400\n",
      "8/8 [==============================] - 0s 187us/step - loss: 0.0137\n",
      "Epoch 128/400\n",
      "8/8 [==============================] - 0s 157us/step - loss: 0.0138\n",
      "Epoch 129/400\n",
      "8/8 [==============================] - 0s 200us/step - loss: 0.0143\n",
      "Epoch 130/400\n",
      "8/8 [==============================] - 0s 173us/step - loss: 0.0137\n",
      "Epoch 131/400\n",
      "8/8 [==============================] - 0s 150us/step - loss: 0.0143\n",
      "Epoch 132/400\n",
      "8/8 [==============================] - 0s 184us/step - loss: 0.0137\n",
      "Epoch 133/400\n",
      "8/8 [==============================] - 0s 206us/step - loss: 0.0138\n",
      "Epoch 134/400\n",
      "8/8 [==============================] - 0s 182us/step - loss: 0.0143\n",
      "Epoch 135/400\n",
      "8/8 [==============================] - 0s 253us/step - loss: 0.0138\n",
      "Epoch 136/400\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0143\n",
      "Epoch 137/400\n",
      "8/8 [==============================] - 0s 229us/step - loss: 0.0137\n",
      "Epoch 138/400\n",
      "8/8 [==============================] - 0s 171us/step - loss: 0.0138\n",
      "Epoch 139/400\n",
      "8/8 [==============================] - 0s 202us/step - loss: 0.0143\n",
      "Epoch 140/400\n",
      "8/8 [==============================] - 0s 181us/step - loss: 0.0138\n",
      "Epoch 141/400\n",
      "8/8 [==============================] - 0s 201us/step - loss: 0.0143\n",
      "Epoch 142/400\n",
      "8/8 [==============================] - 0s 196us/step - loss: 0.0137\n",
      "Epoch 143/400\n",
      "8/8 [==============================] - 0s 158us/step - loss: 0.0138\n",
      "Epoch 144/400\n",
      "8/8 [==============================] - 0s 241us/step - loss: 0.0143\n",
      "Epoch 145/400\n",
      "8/8 [==============================] - 0s 277us/step - loss: 0.0138\n",
      "Epoch 146/400\n",
      "8/8 [==============================] - 0s 176us/step - loss: 0.0143\n",
      "Epoch 147/400\n",
      "8/8 [==============================] - 0s 171us/step - loss: 0.0137\n",
      "Epoch 148/400\n",
      "8/8 [==============================] - 0s 220us/step - loss: 0.0138\n",
      "Epoch 149/400\n",
      "8/8 [==============================] - 0s 202us/step - loss: 0.0143\n",
      "Epoch 150/400\n",
      "8/8 [==============================] - 0s 181us/step - loss: 0.0138\n",
      "Epoch 151/400\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0143\n",
      "Epoch 152/400\n",
      "8/8 [==============================] - 0s 154us/step - loss: 0.0137\n",
      "Epoch 153/400\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0138\n",
      "Epoch 154/400\n",
      "8/8 [==============================] - 0s 164us/step - loss: 0.0143\n",
      "Epoch 155/400\n",
      "8/8 [==============================] - 0s 200us/step - loss: 0.0138\n",
      "Epoch 156/400\n",
      "8/8 [==============================] - 0s 222us/step - loss: 0.0143\n",
      "Epoch 157/400\n",
      "8/8 [==============================] - 0s 183us/step - loss: 0.0137\n",
      "Epoch 158/400\n",
      "8/8 [==============================] - 0s 146us/step - loss: 0.0143\n",
      "Epoch 159/400\n",
      "8/8 [==============================] - 0s 148us/step - loss: 0.0137\n",
      "Epoch 160/400\n",
      "8/8 [==============================] - 0s 227us/step - loss: 0.0138\n",
      "Epoch 161/400\n",
      "8/8 [==============================] - 0s 196us/step - loss: 0.0143\n",
      "Epoch 162/400\n",
      "8/8 [==============================] - 0s 198us/step - loss: 0.0137\n",
      "Epoch 163/400\n",
      "8/8 [==============================] - 0s 181us/step - loss: 0.0143\n",
      "Epoch 164/400\n",
      "8/8 [==============================] - 0s 193us/step - loss: 0.0137\n",
      "Epoch 165/400\n",
      "8/8 [==============================] - 0s 212us/step - loss: 0.0138\n",
      "Epoch 166/400\n",
      "8/8 [==============================] - 0s 202us/step - loss: 0.0143\n",
      "Epoch 167/400\n",
      "8/8 [==============================] - 0s 196us/step - loss: 0.0137\n",
      "Epoch 168/400\n",
      "8/8 [==============================] - 0s 234us/step - loss: 0.0143\n",
      "Epoch 169/400\n",
      "8/8 [==============================] - 0s 213us/step - loss: 0.0137\n",
      "Epoch 170/400\n",
      "8/8 [==============================] - 0s 180us/step - loss: 0.0138\n",
      "Epoch 171/400\n",
      "8/8 [==============================] - 0s 197us/step - loss: 0.0143\n",
      "Epoch 172/400\n",
      "8/8 [==============================] - 0s 259us/step - loss: 0.0137\n",
      "Epoch 173/400\n",
      "8/8 [==============================] - 0s 182us/step - loss: 0.0143\n",
      "Epoch 174/400\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0137\n",
      "Epoch 175/400\n",
      "8/8 [==============================] - 0s 185us/step - loss: 0.0138\n",
      "Epoch 176/400\n",
      "8/8 [==============================] - 0s 185us/step - loss: 0.0143\n",
      "Epoch 177/400\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0137\n",
      "Epoch 178/400\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0143\n",
      "Epoch 179/400\n",
      "8/8 [==============================] - 0s 200us/step - loss: 0.0137\n",
      "Epoch 180/400\n",
      "8/8 [==============================] - 0s 182us/step - loss: 0.0138\n",
      "Epoch 181/400\n",
      "8/8 [==============================] - 0s 179us/step - loss: 0.0147\n",
      "Epoch 182/400\n",
      "8/8 [==============================] - 0s 178us/step - loss: 0.0141\n",
      "Epoch 183/400\n",
      "8/8 [==============================] - 0s 209us/step - loss: 0.0139\n",
      "Epoch 184/400\n",
      "8/8 [==============================] - 0s 241us/step - loss: 0.0146\n",
      "Epoch 185/400\n",
      "8/8 [==============================] - 0s 202us/step - loss: 0.0140\n",
      "Epoch 186/400\n",
      "8/8 [==============================] - 0s 228us/step - loss: 0.0139\n",
      "Epoch 187/400\n",
      "8/8 [==============================] - 0s 159us/step - loss: 0.0145\n",
      "Epoch 188/400\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0140\n",
      "Epoch 189/400\n",
      "8/8 [==============================] - 0s 316us/step - loss: 0.0140\n",
      "Epoch 190/400\n",
      "8/8 [==============================] - 0s 234us/step - loss: 0.0144\n",
      "Epoch 191/400\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0139\n",
      "Epoch 192/400\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0141\n",
      "Epoch 193/400\n",
      "8/8 [==============================] - 0s 216us/step - loss: 0.0143\n",
      "Epoch 194/400\n",
      "8/8 [==============================] - 0s 189us/step - loss: 0.0138\n",
      "Epoch 195/400\n",
      "8/8 [==============================] - 0s 195us/step - loss: 0.0142\n",
      "Epoch 196/400\n",
      "8/8 [==============================] - 0s 261us/step - loss: 0.0142\n",
      "Epoch 197/400\n",
      "8/8 [==============================] - 0s 337us/step - loss: 0.0138\n",
      "Epoch 198/400\n",
      "8/8 [==============================] - 0s 148us/step - loss: 0.0143\n",
      "Epoch 199/400\n",
      "8/8 [==============================] - 0s 165us/step - loss: 0.0142\n",
      "Epoch 200/400\n",
      "8/8 [==============================] - 0s 217us/step - loss: 0.0137\n",
      "Epoch 201/400\n",
      "8/8 [==============================] - 0s 190us/step - loss: 0.0137\n",
      "Epoch 202/400\n",
      "8/8 [==============================] - 0s 213us/step - loss: 0.0142\n",
      "Epoch 203/400\n",
      "8/8 [==============================] - 0s 182us/step - loss: 0.0137\n",
      "Epoch 204/400\n",
      "8/8 [==============================] - 0s 206us/step - loss: 0.0138\n",
      "Epoch 205/400\n",
      "8/8 [==============================] - 0s 186us/step - loss: 0.0142\n",
      "Epoch 206/400\n",
      "8/8 [==============================] - 0s 198us/step - loss: 0.0137\n",
      "Epoch 207/400\n",
      "8/8 [==============================] - 0s 222us/step - loss: 0.0142\n",
      "Epoch 208/400\n",
      "8/8 [==============================] - 0s 156us/step - loss: 0.0137\n",
      "Epoch 209/400\n",
      "8/8 [==============================] - 0s 235us/step - loss: 0.0142\n",
      "Epoch 210/400\n",
      "8/8 [==============================] - 0s 206us/step - loss: 0.0137\n",
      "Epoch 211/400\n",
      "8/8 [==============================] - 0s 183us/step - loss: 0.0138\n",
      "Epoch 212/400\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0142\n",
      "Epoch 213/400\n",
      "8/8 [==============================] - 0s 177us/step - loss: 0.0137\n",
      "Epoch 214/400\n",
      "8/8 [==============================] - 0s 200us/step - loss: 0.0142\n",
      "Epoch 215/400\n",
      "8/8 [==============================] - 0s 218us/step - loss: 0.0137\n",
      "Epoch 216/400\n",
      "8/8 [==============================] - 0s 237us/step - loss: 0.0143\n",
      "Epoch 217/400\n",
      "8/8 [==============================] - 0s 233us/step - loss: 0.0137\n",
      "Epoch 218/400\n",
      "8/8 [==============================] - 0s 218us/step - loss: 0.0138\n",
      "Epoch 219/400\n",
      "8/8 [==============================] - 0s 204us/step - loss: 0.0142\n",
      "Epoch 220/400\n",
      "8/8 [==============================] - 0s 212us/step - loss: 0.0137\n",
      "Epoch 221/400\n",
      "8/8 [==============================] - 0s 311us/step - loss: 0.0143\n",
      "Epoch 222/400\n",
      "8/8 [==============================] - 0s 190us/step - loss: 0.0137\n",
      "Epoch 223/400\n",
      "8/8 [==============================] - 0s 213us/step - loss: 0.0138\n",
      "Epoch 224/400\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0143\n",
      "Epoch 225/400\n",
      "8/8 [==============================] - 0s 190us/step - loss: 0.0137\n",
      "Epoch 226/400\n",
      "8/8 [==============================] - 0s 206us/step - loss: 0.0143\n",
      "Epoch 227/400\n",
      "8/8 [==============================] - 0s 233us/step - loss: 0.0137\n",
      "Epoch 228/400\n",
      "8/8 [==============================] - 0s 219us/step - loss: 0.0138\n",
      "Epoch 229/400\n",
      "8/8 [==============================] - 0s 250us/step - loss: 0.0147\n",
      "Epoch 230/400\n",
      "8/8 [==============================] - 0s 175us/step - loss: 0.0141\n",
      "Epoch 231/400\n",
      "8/8 [==============================] - 0s 233us/step - loss: 0.0139\n",
      "Epoch 232/400\n",
      "8/8 [==============================] - 0s 224us/step - loss: 0.0146\n",
      "Epoch 233/400\n",
      "8/8 [==============================] - 0s 201us/step - loss: 0.0140\n",
      "Epoch 234/400\n",
      "8/8 [==============================] - 0s 167us/step - loss: 0.0140\n",
      "Epoch 235/400\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0144\n",
      "Epoch 236/400\n",
      "8/8 [==============================] - 0s 159us/step - loss: 0.0139\n",
      "Epoch 237/400\n",
      "8/8 [==============================] - 0s 147us/step - loss: 0.0141\n",
      "Epoch 238/400\n",
      "8/8 [==============================] - 0s 201us/step - loss: 0.0144\n",
      "Epoch 239/400\n",
      "8/8 [==============================] - 0s 154us/step - loss: 0.0139\n",
      "Epoch 240/400\n",
      "8/8 [==============================] - 0s 470us/step - loss: 0.0142\n",
      "Epoch 241/400\n",
      "8/8 [==============================] - 0s 211us/step - loss: 0.0143\n",
      "Epoch 242/400\n",
      "8/8 [==============================] - 0s 208us/step - loss: 0.0138\n",
      "Epoch 243/400\n",
      "8/8 [==============================] - 0s 205us/step - loss: 0.0143\n",
      "Epoch 244/400\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0142\n",
      "Epoch 245/400\n",
      "8/8 [==============================] - 0s 221us/step - loss: 0.0138\n",
      "Epoch 246/400\n",
      "8/8 [==============================] - 0s 186us/step - loss: 0.0144\n",
      "Epoch 247/400\n",
      "8/8 [==============================] - 0s 179us/step - loss: 0.0141\n",
      "Epoch 248/400\n",
      "8/8 [==============================] - 0s 185us/step - loss: 0.0137\n",
      "Epoch 249/400\n",
      "8/8 [==============================] - 0s 162us/step - loss: 0.0137\n",
      "Epoch 250/400\n",
      "8/8 [==============================] - 0s 184us/step - loss: 0.0142\n",
      "Epoch 251/400\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0137\n",
      "Epoch 252/400\n",
      "8/8 [==============================] - 0s 191us/step - loss: 0.0142\n",
      "Epoch 253/400\n",
      "8/8 [==============================] - 0s 165us/step - loss: 0.0137\n",
      "Epoch 254/400\n",
      "8/8 [==============================] - 0s 163us/step - loss: 0.0138\n",
      "Epoch 255/400\n",
      "8/8 [==============================] - 0s 146us/step - loss: 0.0142\n",
      "Epoch 256/400\n",
      "8/8 [==============================] - 0s 154us/step - loss: 0.0137\n",
      "Epoch 257/400\n",
      "8/8 [==============================] - 0s 167us/step - loss: 0.0142\n",
      "Epoch 258/400\n",
      "8/8 [==============================] - 0s 160us/step - loss: 0.0137\n",
      "Epoch 259/400\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0142\n",
      "Epoch 260/400\n",
      "8/8 [==============================] - 0s 178us/step - loss: 0.0137\n",
      "Epoch 261/400\n",
      "8/8 [==============================] - 0s 202us/step - loss: 0.0138\n",
      "Epoch 262/400\n",
      "8/8 [==============================] - 0s 218us/step - loss: 0.0142\n",
      "Epoch 263/400\n",
      "8/8 [==============================] - 0s 212us/step - loss: 0.0137\n",
      "Epoch 264/400\n",
      "8/8 [==============================] - 0s 312us/step - loss: 0.0142\n",
      "Epoch 265/400\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0137\n",
      "Epoch 266/400\n",
      "8/8 [==============================] - 0s 170us/step - loss: 0.0143\n",
      "Epoch 267/400\n",
      "8/8 [==============================] - 0s 179us/step - loss: 0.0137\n",
      "Epoch 268/400\n",
      "8/8 [==============================] - 0s 213us/step - loss: 0.0137\n",
      "Epoch 269/400\n",
      "8/8 [==============================] - 0s 284us/step - loss: 0.0143\n",
      "Epoch 270/400\n",
      "8/8 [==============================] - 0s 250us/step - loss: 0.0137\n",
      "Epoch 271/400\n",
      "8/8 [==============================] - 0s 297us/step - loss: 0.0143\n",
      "Epoch 272/400\n",
      "8/8 [==============================] - 0s 202us/step - loss: 0.0137\n",
      "Epoch 273/400\n",
      "8/8 [==============================] - 0s 160us/step - loss: 0.0138\n",
      "Epoch 274/400\n",
      "8/8 [==============================] - 0s 147us/step - loss: 0.0143\n",
      "Epoch 275/400\n",
      "8/8 [==============================] - 0s 178us/step - loss: 0.0137\n",
      "Epoch 276/400\n",
      "8/8 [==============================] - 0s 172us/step - loss: 0.0143\n",
      "Epoch 277/400\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0137\n",
      "Epoch 278/400\n",
      "8/8 [==============================] - 0s 201us/step - loss: 0.0138\n",
      "Epoch 279/400\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0147\n",
      "Epoch 280/400\n",
      "8/8 [==============================] - 0s 211us/step - loss: 0.0141\n",
      "Epoch 281/400\n",
      "8/8 [==============================] - 0s 321us/step - loss: 0.0138\n",
      "Epoch 282/400\n",
      "8/8 [==============================] - 0s 317us/step - loss: 0.0146\n",
      "Epoch 283/400\n",
      "8/8 [==============================] - 0s 195us/step - loss: 0.0140\n",
      "Epoch 284/400\n",
      "8/8 [==============================] - 0s 163us/step - loss: 0.0139\n",
      "Epoch 285/400\n",
      "8/8 [==============================] - 0s 191us/step - loss: 0.0145\n",
      "Epoch 286/400\n",
      "8/8 [==============================] - 0s 284us/step - loss: 0.0140\n",
      "Epoch 287/400\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0140\n",
      "Epoch 288/400\n",
      "8/8 [==============================] - 0s 146us/step - loss: 0.0144\n",
      "Epoch 289/400\n",
      "8/8 [==============================] - 0s 161us/step - loss: 0.0139\n",
      "Epoch 290/400\n",
      "8/8 [==============================] - 0s 156us/step - loss: 0.0141\n",
      "Epoch 291/400\n",
      "8/8 [==============================] - 0s 193us/step - loss: 0.0143\n",
      "Epoch 292/400\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0138\n",
      "Epoch 293/400\n",
      "8/8 [==============================] - 0s 178us/step - loss: 0.0142\n",
      "Epoch 294/400\n",
      "8/8 [==============================] - 0s 203us/step - loss: 0.0142\n",
      "Epoch 295/400\n",
      "8/8 [==============================] - 0s 207us/step - loss: 0.0138\n",
      "Epoch 296/400\n",
      "8/8 [==============================] - 0s 216us/step - loss: 0.0143\n",
      "Epoch 297/400\n",
      "8/8 [==============================] - 0s 165us/step - loss: 0.0142\n",
      "Epoch 298/400\n",
      "8/8 [==============================] - 0s 233us/step - loss: 0.0137\n",
      "Epoch 299/400\n",
      "8/8 [==============================] - 0s 199us/step - loss: 0.0144\n",
      "Epoch 300/400\n",
      "8/8 [==============================] - 0s 220us/step - loss: 0.0141\n",
      "Epoch 301/400\n",
      "8/8 [==============================] - 0s 228us/step - loss: 0.0137\n",
      "Epoch 302/400\n",
      "8/8 [==============================] - 0s 167us/step - loss: 0.0141\n",
      "Epoch 303/400\n",
      "8/8 [==============================] - 0s 211us/step - loss: 0.0137\n",
      "Epoch 304/400\n",
      "8/8 [==============================] - 0s 180us/step - loss: 0.0142\n",
      "Epoch 305/400\n",
      "8/8 [==============================] - 0s 221us/step - loss: 0.0137\n",
      "Epoch 306/400\n",
      "8/8 [==============================] - 0s 172us/step - loss: 0.0137\n",
      "Epoch 307/400\n",
      "8/8 [==============================] - 0s 175us/step - loss: 0.0142\n",
      "Epoch 308/400\n",
      "8/8 [==============================] - 0s 163us/step - loss: 0.0137\n",
      "Epoch 309/400\n",
      "8/8 [==============================] - 0s 197us/step - loss: 0.0142\n",
      "Epoch 310/400\n",
      "8/8 [==============================] - 0s 159us/step - loss: 0.0137\n",
      "Epoch 311/400\n",
      "8/8 [==============================] - 0s 220us/step - loss: 0.0142\n",
      "Epoch 312/400\n",
      "8/8 [==============================] - 0s 207us/step - loss: 0.0137\n",
      "Epoch 313/400\n",
      "8/8 [==============================] - 0s 172us/step - loss: 0.0142\n",
      "Epoch 314/400\n",
      "8/8 [==============================] - 0s 184us/step - loss: 0.0137\n",
      "Epoch 315/400\n",
      "8/8 [==============================] - 0s 197us/step - loss: 0.0137\n",
      "Epoch 316/400\n",
      "8/8 [==============================] - 0s 226us/step - loss: 0.0142\n",
      "Epoch 317/400\n",
      "8/8 [==============================] - 0s 177us/step - loss: 0.0137\n",
      "Epoch 318/400\n",
      "8/8 [==============================] - 0s 359us/step - loss: 0.0143\n",
      "Epoch 319/400\n",
      "8/8 [==============================] - 0s 154us/step - loss: 0.0137\n",
      "Epoch 320/400\n",
      "8/8 [==============================] - 0s 155us/step - loss: 0.0138\n",
      "Epoch 321/400\n",
      "8/8 [==============================] - 0s 144us/step - loss: 0.0142\n",
      "Epoch 322/400\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0137\n",
      "Epoch 323/400\n",
      "8/8 [==============================] - 0s 154us/step - loss: 0.0143\n",
      "Epoch 324/400\n",
      "8/8 [==============================] - 0s 156us/step - loss: 0.0137\n",
      "Epoch 325/400\n",
      "8/8 [==============================] - 0s 207us/step - loss: 0.0143\n",
      "Epoch 326/400\n",
      "8/8 [==============================] - 0s 177us/step - loss: 0.0137\n",
      "Epoch 327/400\n",
      "8/8 [==============================] - 0s 149us/step - loss: 0.0137\n",
      "Epoch 328/400\n",
      "8/8 [==============================] - 0s 190us/step - loss: 0.0143\n",
      "Epoch 329/400\n",
      "8/8 [==============================] - 0s 200us/step - loss: 0.0137\n",
      "Epoch 330/400\n",
      "8/8 [==============================] - 0s 238us/step - loss: 0.0143\n",
      "Epoch 331/400\n",
      "8/8 [==============================] - 0s 162us/step - loss: 0.0137\n",
      "Epoch 332/400\n",
      "8/8 [==============================] - 0s 183us/step - loss: 0.0138\n",
      "Epoch 333/400\n",
      "8/8 [==============================] - 0s 181us/step - loss: 0.0143\n",
      "Epoch 334/400\n",
      "8/8 [==============================] - 0s 176us/step - loss: 0.0137\n",
      "Epoch 335/400\n",
      "8/8 [==============================] - 0s 181us/step - loss: 0.0143\n",
      "Epoch 336/400\n",
      "8/8 [==============================] - 0s 173us/step - loss: 0.0137\n",
      "Epoch 337/400\n",
      "8/8 [==============================] - 0s 181us/step - loss: 0.0138\n",
      "Epoch 338/400\n",
      "8/8 [==============================] - 0s 204us/step - loss: 0.0147\n",
      "Epoch 339/400\n",
      "8/8 [==============================] - 0s 191us/step - loss: 0.0141\n",
      "Epoch 340/400\n",
      "8/8 [==============================] - 0s 232us/step - loss: 0.0138\n",
      "Epoch 341/400\n",
      "8/8 [==============================] - 0s 176us/step - loss: 0.0146\n",
      "Epoch 342/400\n",
      "8/8 [==============================] - 0s 234us/step - loss: 0.0140\n",
      "Epoch 343/400\n",
      "8/8 [==============================] - 0s 178us/step - loss: 0.0139\n",
      "Epoch 344/400\n",
      "8/8 [==============================] - 0s 208us/step - loss: 0.0145\n",
      "Epoch 345/400\n",
      "8/8 [==============================] - 0s 180us/step - loss: 0.0140\n",
      "Epoch 346/400\n",
      "8/8 [==============================] - 0s 190us/step - loss: 0.0140\n",
      "Epoch 347/400\n",
      "8/8 [==============================] - 0s 177us/step - loss: 0.0144\n",
      "Epoch 348/400\n",
      "8/8 [==============================] - 0s 170us/step - loss: 0.0139\n",
      "Epoch 349/400\n",
      "8/8 [==============================] - 0s 177us/step - loss: 0.0141\n",
      "Epoch 350/400\n",
      "8/8 [==============================] - 0s 171us/step - loss: 0.0143\n",
      "Epoch 351/400\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0138\n",
      "Epoch 352/400\n",
      "8/8 [==============================] - 0s 184us/step - loss: 0.0142\n",
      "Epoch 353/400\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0142\n",
      "Epoch 354/400\n",
      "8/8 [==============================] - 0s 210us/step - loss: 0.0138\n",
      "Epoch 355/400\n",
      "8/8 [==============================] - 0s 207us/step - loss: 0.0143\n",
      "Epoch 356/400\n",
      "8/8 [==============================] - 0s 215us/step - loss: 0.0142\n",
      "Epoch 357/400\n",
      "8/8 [==============================] - 0s 245us/step - loss: 0.0137\n",
      "Epoch 358/400\n",
      "8/8 [==============================] - 0s 263us/step - loss: 0.0144\n",
      "Epoch 359/400\n",
      "8/8 [==============================] - 0s 329us/step - loss: 0.0141\n",
      "Epoch 360/400\n",
      "8/8 [==============================] - 0s 213us/step - loss: 0.0137\n",
      "Epoch 361/400\n",
      "8/8 [==============================] - 0s 185us/step - loss: 0.0141\n",
      "Epoch 362/400\n",
      "8/8 [==============================] - 0s 223us/step - loss: 0.0137\n",
      "Epoch 363/400\n",
      "8/8 [==============================] - 0s 184us/step - loss: 0.0142\n",
      "Epoch 364/400\n",
      "8/8 [==============================] - 0s 173us/step - loss: 0.0137\n",
      "Epoch 365/400\n",
      "8/8 [==============================] - 0s 165us/step - loss: 0.0137\n",
      "Epoch 366/400\n",
      "8/8 [==============================] - 0s 153us/step - loss: 0.0142\n",
      "Epoch 367/400\n",
      "8/8 [==============================] - 0s 164us/step - loss: 0.0137\n",
      "Epoch 368/400\n",
      "8/8 [==============================] - 0s 186us/step - loss: 0.0142\n",
      "Epoch 369/400\n",
      "8/8 [==============================] - 0s 220us/step - loss: 0.0137\n",
      "Epoch 370/400\n",
      "8/8 [==============================] - 0s 225us/step - loss: 0.0142\n",
      "Epoch 371/400\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0137\n",
      "Epoch 372/400\n",
      "8/8 [==============================] - 0s 151us/step - loss: 0.0137\n",
      "Epoch 373/400\n",
      "8/8 [==============================] - 0s 309us/step - loss: 0.0142\n",
      "Epoch 374/400\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0137\n",
      "Epoch 375/400\n",
      "8/8 [==============================] - 0s 173us/step - loss: 0.0142\n",
      "Epoch 376/400\n",
      "8/8 [==============================] - 0s 223us/step - loss: 0.0137\n",
      "Epoch 377/400\n",
      "8/8 [==============================] - 0s 172us/step - loss: 0.0143\n",
      "Epoch 378/400\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0137\n",
      "Epoch 379/400\n",
      "8/8 [==============================] - 0s 217us/step - loss: 0.0137\n",
      "Epoch 380/400\n",
      "8/8 [==============================] - 0s 180us/step - loss: 0.0142\n",
      "Epoch 381/400\n",
      "8/8 [==============================] - 0s 265us/step - loss: 0.0137\n",
      "Epoch 382/400\n",
      "8/8 [==============================] - 0s 191us/step - loss: 0.0143\n",
      "Epoch 383/400\n",
      "8/8 [==============================] - 0s 247us/step - loss: 0.0137\n",
      "Epoch 384/400\n",
      "8/8 [==============================] - 0s 184us/step - loss: 0.0138\n",
      "Epoch 385/400\n",
      "8/8 [==============================] - 0s 250us/step - loss: 0.0147\n",
      "Epoch 386/400\n",
      "8/8 [==============================] - 0s 164us/step - loss: 0.0141\n",
      "Epoch 387/400\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0138\n",
      "Epoch 388/400\n",
      "8/8 [==============================] - 0s 188us/step - loss: 0.0146\n",
      "Epoch 389/400\n",
      "8/8 [==============================] - 0s 181us/step - loss: 0.0140\n",
      "Epoch 390/400\n",
      "8/8 [==============================] - 0s 191us/step - loss: 0.0139\n",
      "Epoch 391/400\n",
      "8/8 [==============================] - 0s 194us/step - loss: 0.0144\n",
      "Epoch 392/400\n",
      "8/8 [==============================] - 0s 208us/step - loss: 0.0139\n",
      "Epoch 393/400\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0140\n",
      "Epoch 394/400\n",
      "8/8 [==============================] - 0s 153us/step - loss: 0.0144\n",
      "Epoch 395/400\n",
      "8/8 [==============================] - 0s 194us/step - loss: 0.0139\n",
      "Epoch 396/400\n",
      "8/8 [==============================] - 0s 189us/step - loss: 0.0141\n",
      "Epoch 397/400\n",
      "8/8 [==============================] - 0s 256us/step - loss: 0.0143\n",
      "Epoch 398/400\n",
      "8/8 [==============================] - 0s 243us/step - loss: 0.0138\n",
      "Epoch 399/400\n",
      "8/8 [==============================] - 0s 203us/step - loss: 0.0142\n",
      "Epoch 400/400\n",
      "8/8 [==============================] - 0s 282us/step - loss: 0.0142\n",
      "best epoch =  364\n",
      "smallest loss = 0.013678543269634247\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=400,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3d300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2245948 ]\n",
      " [0.30143216]\n",
      " [0.72449493]]\n",
      "w01 =  1.2245948 w02 =  0.30143216 w03 =  0.72449493\n",
      "[-0.13787363]\n",
      "b1 =  [-0.13787363]\n",
      "[[0.7087467]]\n",
      "w12 =  0.7087467\n",
      "[-0.10063175]\n",
      "b2 =  [-0.10063175]\n",
      "[[0.68293154]]\n",
      "w23 =  0.68293154\n",
      "[0.03686729]\n",
      "b3 =  [0.03686729]\n",
      "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9558346964599856 [[0.9729444]]\n",
      "0.9900990099009901 1.0 1.0 0.9968828122588808 [[0.9848495]]\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9721922162896206 [[0.99062216]]\n",
      "1.0 0.896551724137931 1.009090909090909 0.9539829017622912 [[0.97881293]]\n",
      "0.9900990099009901 1.0 1.0 1.003055461251196 [[0.9848495]]\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.99649084]]\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.0984228881824636 [[1.0903177]]\n",
      "1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.4543426]]\n",
      "  \n",
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "20.0 13.0 310.8 30.969044165303533 [[31.5234]]\n",
      "20.0 14.5 308.0 32.29900311718774 [[31.909126]]\n",
      "20.0 15.3 306.0 31.499027807783705 [[32.09616]]\n",
      "20.2 13.0 310.8 30.909046017098234 [[31.713541]]\n",
      "20.0 14.5 308.0 32.498996944538746 [[31.909126]]\n",
      "20.2 15.3 306.0 31.3990308941082 [[32.286304]]\n",
      "23.999999999999996 13.0 310.8 35.58890157711182 [[35.326298]]\n",
      "36.0 14.5 308.0 46.398567945433776 [[47.1207]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray= np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f099d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''>>>>> start CodeP2.3F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Data arrays for hybrid solar/fossil-fuel gas turbine power system'''\n",
    "\n",
    "'''\n",
    "#create input data array, normalizing input temp\n",
    "#T1(K), gamma, , qsol(kW):\n",
    "xdata = []\n",
    "xdata =  [[ 318.0 , 0.0 , 500.0 ], [ 318.0 , 0.0 , 1000.0 ]]\n",
    "xdata.append([ 318.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 318.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 318.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 2500.0 ])\n",
    "  \n",
    "xdata.append([ 303.0 , 0.0 , 500.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 1000.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 2500.0 ])\n",
    "  \n",
    "xdata.append([ 288.0 , 0.0 , 500.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 1000.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 2500.0 ])\n",
    "  \n",
    "xdata.append([ 268.0 , 0.0 , 500.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 1000.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 2500.0 ])\n",
    "  '''\n",
    "\n",
    "'''\n",
    "ydata =  [[ 35.1316 , 0.3808 ],[ 40.3764 , 0.38686 ]]\n",
    "ydata.append([ 47.4620 , 0.3930 ])\n",
    "ydata.append([ 57.5639 , 0.39949 ])\n",
    "ydata.append([ 73.1286 , 0.40612 ])\n",
    "ydata.append([ 49.1110 , 0.4023 ])\n",
    "ydata.append([ 56.4428 , 0.40605 ])\n",
    "ydata.append([ 66.3479 , 0.4098 ])\n",
    "ydata.append([ 80.4695 , 0.413 ])\n",
    "ydata.append([ 102.2276 , 0.4175 ])\n",
    "ydata.append([ 63.0904 , 0.41540 ])\n",
    "ydata.append([ 72.5092 , 0.4175 ])\n",
    "ydata.append([ 85.2338, 0.4197 ])\n",
    "ydata.append([ 103.3750 , 0.42192 ])\n",
    "ydata.append([ 131.3266 , 0.4242 ])\n",
    "  \n",
    "ydata.append([ 34.273 , 0.3952 ])\n",
    "ydata.append([ 38.99026 , 0.4012 ])\n",
    "ydata.append([ 45.2133, 0.4073 ])\n",
    "ydata.append([ 53.8000 , 0.4136 ])\n",
    "ydata.append([ 66.4130 , 0.4201 ])\n",
    "ydata.append([ 47.922 , 0.4178 ])\n",
    "ydata.append([ 54.518 , 0.4215 ])\n",
    "ydata.append([ 63.220 , 0.4252 ])\n",
    "ydata.append([ 75.226 , 0.4290 ])\n",
    "ydata.append([ 92.862 , 0.4329 ])\n",
    "ydata.append([ 61.572 , 0.4315 ])\n",
    "ydata.append([ 70.0468 , 0.43373 ])\n",
    "ydata.append([ 81.226 , 0.43597 ])\n",
    "ydata.append([ 96.653 , 0.4382 ])\n",
    "ydata.append([ 119.3124 , 0.44045 ])\n",
    "  \n",
    "ydata.append([ 33.4521 , 0.40913 ])\n",
    "ydata.append([ 37.6911, 0.4150 ])\n",
    "ydata.append([ 43.1602 , 0.4209 ])\n",
    "ydata.append([ 50.4858 , 0.4271 ])\n",
    "ydata.append([ 60.8067 , 0.4334 ])\n",
    "ydata.append([ 46.7865 , 0.4328 ])\n",
    "ydata.append([ 52.7151 , 0.43646 ])\n",
    "ydata.append([ 60.36425 , 0.44016 ])\n",
    "ydata.append([ 70.6099 , 0.443926 ])\n",
    "ydata.append([ 85.0447 , 0.4477 ])\n",
    "ydata.append([ 60.1208 , 0.44721 ])\n",
    "ydata.append([ 67.7391 , 0.44940 ])\n",
    "ydata.append([ 77.56830 , 0.4516 ])\n",
    "ydata.append([ 90.73410 , 0.4538 ])\n",
    "ydata.append([ 109.2828 , 0.4560 ])\n",
    "  \n",
    "ydata.append([ 32.4123 , 0.42694 ])\n",
    "ydata.append([ 36.0807 , 0.4325 ])\n",
    "ydata.append([ 40.6854 , 0.4383 ])\n",
    "ydata.append([ 46.6374 , 0.4442 ])\n",
    "ydata.append([ 54.6293 , 0.4503 ])\n",
    "ydata.append([ 45.3472 , 0.4519 ])\n",
    "ydata.append([ 50.4796 , 0.4555 ])\n",
    "ydata.append([ 56.9219 , 0.4591 ])\n",
    "ydata.append([ 65.2492 , 0.4628 ])\n",
    "ydata.append([ 76.4304 , 0.4665 ])\n",
    "ydata.append([ 58.2822 , 0.4672 ])\n",
    "ydata.append([ 64.8785 , 0.4693 ])\n",
    "ydata.append([ 73.1584 , 0.4715 ])\n",
    "ydata.append([ 83.8610 , 0.4738 ])\n",
    "ydata.append([ 98.2316 , 0.4760 ])\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db33a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''>>>>> start CodeP2.4F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for hybrid solar/fossil-fuel gas turbine power system'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#create input data array\n",
    "# meadian values of input variables\n",
    "Tmed = 293.\n",
    "gamed = 0.25\n",
    "qsmed = 1250.\n",
    "#T1(K), gamma, , qsol(kW):\n",
    "xdata = []\n",
    "xdata =  [[ 318.0, 0.0, 500.0], [ 318.0, 0.0, 1000.0]]\n",
    "xdata.append([ 318.0, 0.0, 1500.0])\n",
    "xdata.append([ 318.0, 0.0, 2000.0])\n",
    "xdata.append([ 318.0, 0.0, 2500.0])\n",
    "'''#convert to:\n",
    "xdata =  [[ 318.0/Tmed , 0.0/gamed , 500.0/qsmed ], [ 318.0/Tmed , 0.0/gamed , 1000.0/qsmed ]]\n",
    "xdata.append([ 318.0/Tmed  , 0.0/gamed , 1500.0/qsmed ])\n",
    "xdata.append([ 318.0/Tmed  , 0.0/gamed , 2000.0/qsmed ])\n",
    "xdata.append([ 318.0/Tmed  , 0.0/gamed , 2500.0/qsmed ])'''\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)\n",
    "# meadian values of output variables\n",
    "almed = 60.\n",
    "efmed = 0.4\n",
    "# alpha, effsys\n",
    "ydata = []\n",
    "ydata =  [[ 35.1316, 0.3808], [ 40.3764, 0.38686]]\n",
    "ydata.append([ 47.4620, 0.3930])\n",
    "ydata.append([ 57.5639, 0.39949])\n",
    "ydata.append([ 73.1286, 0.40612])\n",
    "'''#convert to:\n",
    "ydata =  [[ 35.1316/almed , 0.3808/efmed ], [ 40.3764/almed , 0.38686/efmed ]]\n",
    "ydata.append([ 47.4620/almed , 0.3930/efmed ])\n",
    "ydata.append([ 57.5639/almed , 0.39949/efmed ])\n",
    "ydata.append([ 73.1286/almed , 0.40612/efmed ])'''\n",
    "\n",
    "\n",
    "yarray= np.array(ydata)\n",
    "print (ydata)\n",
    "print (yarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0febc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "\n",
    "#As seen below, we have created four dense layers. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 1 in our case. \n",
    "#The activation function we have chosen is elu, which stands for exponential linear unit. .\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=0.5)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=K.elu, input_shape=[3],  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(32, activation=K.elu,  kernel_initializer=initializer),\n",
    "    '''in Task 2.2, add 3rd layer here with 16 neurons'''\n",
    "    keras.layers.Dense(2,  kernel_initializer=initializer)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean squared error. After the compilation of the model, we’ll use the fit method with ~500 epochs.\n",
    "#Number of epochs can be varied.\n",
    "\n",
    "#from tf.keras import optimizers\n",
    "rms = keras.optimizers.RMSprop(0.020)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faae4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,yarray,epochs=800,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab49f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "outpt=[]\n",
    "\n",
    "#first point (row [0])comparison of data and prediction\n",
    "# put in a loop to print comparion for all data points\n",
    "\n",
    "test = [[ xarray[0][0] , xarray[0][1] , xarray[0][2] ]]\n",
    "testarray = np.array(test)\n",
    "outpt = model.predict(testarray)\n",
    "print ('row [0] data:  T1= ', xarray[0][0]*Tmed, ', gam= ', xarray[0][1]*gamed, \\\n",
    "    ', qsol= ', xarray[0][2]*qsmed,', alpha= ', yarray[0][0]*almed,\\\n",
    "    ',  predicted alpha = ', outpt[0][0]*almed)\n",
    "\n",
    "#20th point (row [20])comparison of data and prediction\n",
    "test = [[ xarray[20][0] , xarray[20][1] , xarray[20][2] ]]\n",
    "testarray = np.array(test)\n",
    "outpt = model.predict(testarray)\n",
    "print ('row [20] data:  T1= ', xarray[20][0]*Tmed, ', gam= ', xarray[0][1]*gamed, \\\n",
    "    ', qsol= ', xarray[20][2]*qsmed,', alpha= ', yarray[20][0]*almed,\\\n",
    "    ',  predicted alpha = ', outpt[0][0]*almed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029e58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d40197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
