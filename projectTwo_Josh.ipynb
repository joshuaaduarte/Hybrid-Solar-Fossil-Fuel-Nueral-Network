{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cd8036-5b18-46bd-a2aa-11693b7b5c70",
   "metadata": {},
   "source": [
    "# Project Two\n",
    "## Work Done by Joshua Duarte\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72595b5-cd1b-4d38-8ecc-b28245d3f40e",
   "metadata": {},
   "source": [
    "## Code P2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e6e22-9369-4fcf-99c5-576ebe93bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Intro to Neural Network Modeling \n",
    "# Python Neural Network Model of Spray Cooling Test System\n",
    "\n",
    ">>>>> start CodeP2.1F22\n",
    "    V.P. Carey, ME249, Fall 2022'''\n",
    "\n",
    "# version 3 print function\n",
    "from __future__ import print_function\n",
    "\n",
    "# import math, numpy and other usefuk packages\n",
    "import math\n",
    "import numpy \n",
    "\n",
    "%matplotlib inline\n",
    "# importing the required module \n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = [10, 8] # for square canvas\n",
    "\n",
    "\n",
    "#assembling data array\n",
    "#store array where rows are data vectors [x01, x02, x03, y3]\n",
    "xydata = []\n",
    "\n",
    "xydata = [[20./20.2, 13.0/14.5, 310.8/308.0, 30.97/32.4], [20./20.2, 14.5/14.5, 308.0/308.0, 32.3/32.4]]\n",
    "xydata.append([20./20.2, 15.3/14.5, 306.0/308.0, 31.5/32.4])\n",
    "xydata.append([20.2/20.2, 13.0/14.5, 310.8/308.0, 30.91/32.4]) \n",
    "xydata.append([20./20.2, 14.5/14.5, 308.0/308.0, 32.5/32.4]) \n",
    "xydata.append([20./20.2, 15.3/14.5, 306.0/308.0, 31.4/32.4]) \n",
    "xydata.append([24./20.2, 13.0/14.5, 310.8/308.0, 35.59/32.4]) \n",
    "xydata.append([36./20.2, 14.5/14.5, 308.0/308.0, 46.4/32.4]) \n",
    "print (xydata)\n",
    "\n",
    "#set starting values \n",
    "w01n =  1.23 \n",
    "w02n =  0.40 \n",
    "w03n =  0.70\n",
    "b1n =  -0.15\n",
    "w12n =  0.72\n",
    "b2n =  -0.12\n",
    "w23n =  0.7\n",
    "b3n =  0.01\n",
    "\n",
    "#start of batch loop  \n",
    "\n",
    "for k in range (0,200):\n",
    "    icount = 0\n",
    "    #initialize error and derivative parameters\n",
    "    E3ti = 0.\n",
    "    dE3da3 = 0.\n",
    "    dE3dw01ti = 0.\n",
    "    dE3dw02ti = 0.\n",
    "    dE3dw03ti = 0.\n",
    "    dE3db1ti = 0.\n",
    "    dE3dw12ti = 0.\n",
    "    dE3db2ti = 0.\n",
    "    dE3dw23ti = 0.\n",
    "    dE3db3ti = 0.\n",
    " \n",
    "    w01 = w01n \n",
    "    w02 = w02n\n",
    "    w03 = w03n\n",
    "    b1 = b1n \n",
    "    w12 = w12n\n",
    "    b2 = b2n \n",
    "    w23 = w23n \n",
    "    b3 = b3n \n",
    "    \n",
    "    #doing calcuations for each data point \n",
    "    for i in range(0,8):\n",
    "        #compute activation functions and their derivatives\n",
    "        z1 = w01*xydata[i][0]+w02*xydata[i][1]+w03*xydata[i][2]+b1 \n",
    "        sig1 = z1\n",
    "        sigp1 = 1.0\n",
    "        if z1 < 0.0:\n",
    "            sig1 = math.exp(z1) - 1.0\n",
    "            sigp1 = math.exp(z1)\n",
    "        a1 = sig1\n",
    "\n",
    "        z2 = w12*a1+b2 \n",
    "        sig2 = z2\n",
    "        sigp2 = 1.0\n",
    "        if z2 < 0.0:\n",
    "            sig2 = math.exp(z2) - 1.0\n",
    "            sigp2 = math.exp(z2)\n",
    "        a2 = sig2\n",
    "\n",
    "        z3 = w23*a2+b3 \n",
    "        sig3 = z3\n",
    "        sigp3 = 1.0\n",
    "        if z3 < 0.0:\n",
    "            sig3 = math.exp(z3) - 1.0\n",
    "            sigp3 = math.exp(z3)\n",
    "        a3 = sig3\n",
    "        \n",
    "        \n",
    "        #compute derivatives for backpropagation\n",
    "        #add to sum for batch average calculation\n",
    "        E3ti = E3ti +(a3 - xydata[i][3])*(a3 - xydata[i][3])\n",
    "        dE3da3 = 2.*(a3 - xydata[i][3])\n",
    "        \n",
    "        dE3dw01ti = dE3dw01ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][0]\n",
    "        dE3dw02ti = dE3dw02ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][1]\n",
    "        dE3dw03ti = \n",
    "        dE3db1ti = \n",
    "        \n",
    "        dE3dw12ti = \n",
    "        dE3db2ti = \n",
    "        \n",
    "        dE3dw23ti = \n",
    "        dE3db3ti = \n",
    "        \n",
    "        icount = i + 1\n",
    "        # end  calculations for each data point in batch\n",
    "        \n",
    "    #compute batch averaged values\n",
    "    E3 = E3ti/icount\n",
    "    dE3dw01 = dE3dw01ti/icount\n",
    "    dE3dw02 = dE3dw02ti/icount\n",
    "    dE3dw03 = dE3dw03ti/icount\n",
    "    dE3db1 = dE3db1ti/icount\n",
    "    dE3dw12 = dE3dw12ti/icount\n",
    "    dE3db2 = dE3db2ti/icount\n",
    "    dE3dw23 = dE3dw23ti/icount\n",
    "    dE3db3 = dE3db3ti/icount\n",
    "    \n",
    "    #set gam = learning rate\n",
    "    gam = 0.03\n",
    "    if E3 < 0.07: \n",
    "        gam = 0.009\n",
    "\n",
    "    w01n = w01 + gam*(-E3)/dE3dw01\n",
    "    w02n = w02 + gam*(-E3)/dE3dw02\n",
    "    w03n = w03 + gam*(-E3)/dE3dw03\n",
    "    b1n = b1 + gam*(-E3)/dE3db1\n",
    "    w12n = w12 + gam*(-E3)/dE3dw12\n",
    "    b2n = b2 + gam*(-E3)/dE3db2\n",
    "    \n",
    "    w23n = w23 + gam*(-E3)/dE3dw23\n",
    "    b3n = b3 + gam*(-E3)/dE3db3\n",
    "    \n",
    "    #printing for each iteration\n",
    "    print ('last w01, w02, w03, w12, w23:')\n",
    "    print ('last b1, b2, b3:')\n",
    "    print (w01, w02, w03, w12, w23)\n",
    "    print (b1, b2, b3)\n",
    "    print ('E3 = ', E3, 'icount =', icount)\n",
    "    print ('next ws:', w01n, w02n, w03n, w12n, w23n)\n",
    "    print ('next bs:', b1n, b2n, b3n)\n",
    "    \n",
    "    #quit if squared error is below target\n",
    "    if E3 < 0.001:\n",
    "        break\n",
    "    \n",
    "\n",
    "print ('last w01, w02, w03, w12, w23:')\n",
    "print ('last b1, b2, b3:')\n",
    "print (w01, w02, w03, w12, w23)\n",
    "print (b1, b2, b3)\n",
    "#decomment print statements below if you want to print neuron outputs\n",
    "#print ('z1 =', z1)\n",
    "#print ('a1 =', a1)\n",
    "#print ('z2 =', z2)\n",
    "#print ('a2 =', a2)\n",
    "#print ('z3 =', z3)\n",
    "#print ('a3 =', a3)\n",
    "\n",
    "#print comparison of data and trained network predictions\n",
    "# restore raw data values  \n",
    "xydatar = [[20., 13.0, 310.8, 30.97], [20., 14.5, 308.0, 32.3]]\n",
    "xydatar.append([20., 15.3, 306.0, 31.5])\n",
    "xydatar.append([20.2, 13.0, 310.8, 30.91]) \n",
    "xydatar.append([20., 14.5, 308.0, 32.5]) \n",
    "xydatar.append([20., 15.3, 306.0, 31.4]) \n",
    "xydatar.append([24., 13.0, 310.8, 35.59]) \n",
    "xydatar.append([36., 14.5, 308.0, 46.4])\n",
    "print ('Tdbin, Twbin, qdot, Tdbout, ypredicted:')\n",
    "for i in range(0,8): \n",
    "    z1 = w01*xydata[i][0]+w02*xydata[i][1]+w03*xydata[i][2]+b1 \n",
    "    sig1 = z1\n",
    "    sigp1 = 1.0\n",
    "    if z1 < 0.0:\n",
    "        sig1 = math.exp(z1) - 1.0\n",
    "        sigp1 = math.exp(z1)\n",
    "    a1 = sig1\n",
    "\n",
    "    z2 = w12*a1+b2 \n",
    "    sig2 = z2\n",
    "    sigp2 = 1.0\n",
    "    if z2 < 0.0:\n",
    "        sig2 = math.exp(z2) - 1.0\n",
    "        sigp2 = math.exp(z2)\n",
    "    a2 = sig2\n",
    "\n",
    "    z3 = w23*a2+b3 \n",
    "    sig3 = z3\n",
    "    sigp3 = 1.0\n",
    "    if z3 < 0.0:\n",
    "        sig3 = math.exp(z3) - 1.0\n",
    "        sigp3 = math.exp(z3)\n",
    "    a3 = sig3\n",
    "\n",
    "    print (xydatar[i][0], xydatar[i][1], xydatar[i][2], xydatar[i][3], a3*32.4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca4837-82a9-4740-a274-2dff3535b7e3",
   "metadata": {},
   "source": [
    "## Code P2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b419b2-3d08-415c-a078-f9e3a0989716",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''>>>>> start CodeP2.2F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
    "    'y3' : [30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "\n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37722659-e9b5-4c26-9ac4-3f28ff2fed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3],  kernel_initializer=initializer, name=\"dense_one\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_two\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_three\")\n",
    "  ])\n",
    "\n",
    "\n",
    "#set starting values to those used in first principles model\n",
    "w01n =  1.23 \n",
    "w02n =  0.40 \n",
    "w03n =  0.70\n",
    "b1n =  -0.15\n",
    "w12n =  0.72\n",
    "b2n =  -0.12\n",
    "w23n =  0.7\n",
    "b3n =  0.01\n",
    "\n",
    "weights0 =  [[ w01n], [w02n], [ w03n]]\n",
    "w0array= np.array(weights0)\n",
    "print(np.shape(w0array))\n",
    "bias0 = [b1n]\n",
    "bias0array= np.array(bias0)\n",
    "L0=[]\n",
    "L0.append(w0array)\n",
    "L0.append(bias0array)\n",
    "model.layers[0].set_weights(L0) \n",
    "\n",
    "weights1 =  [[ w12n]]\n",
    "w1array= np.array(weights1)\n",
    "print(np.shape(w1array))\n",
    "bias1 = [b2n]\n",
    "bias1array= np.array(bias1)\n",
    "L1=[]\n",
    "L1.append(w1array)\n",
    "L1.append(bias1array)\n",
    "model.layers[1].set_weights(L1)\n",
    "\n",
    "weights2 =  [[ w23n]]\n",
    "w2array= np.array(weights2)\n",
    "print(np.shape(w2array))\n",
    "bias2 = [b3n]\n",
    "bias2array= np.array(bias2)\n",
    "L2=[]\n",
    "L2.append(w2array)\n",
    "L2.append(bias2array)\n",
    "model.layers[2].set_weights(L2)\n",
    "\n",
    "\n",
    "print(\"Weights and biases of the layers before training the model: \\n\")\n",
    "for layer in model.layers:\n",
    "  print(layer.name)\n",
    "  print(\"Weights\")\n",
    "  print(\"Shape: \",layer.get_weights()[0].shape,'\\n',layer.get_weights()[0])\n",
    "  print(\"Bias\")\n",
    "  print(\"Shape: \",layer.get_weights()[1].shape,'\\n',layer.get_weights()[1],'\\n')\n",
    "model.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597c220-6cb9-47fa-af74-48dce695a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "rms = keras.optimizers.RMSprop(0.0035)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238405ca-6c51-44df-894b-f1945e9e65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=800,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ccc57c-253d-4340-8c76-f10cd13b737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray= np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7b09f-591c-4e83-b847-09519a5afd29",
   "metadata": {},
   "source": [
    "## Code P2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12869b-abf9-46c6-a425-b992c0e23707",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''>>>>> start CodeP2.3F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Data arrays for hybrid solar/fossil-fuel gas turbine power system'''\n",
    "\n",
    "'''\n",
    "#create input data array, normalizing input temp\n",
    "#T1(K), gamma, , qsol(kW):\n",
    "xdata = []\n",
    "xdata =  [[ 318.0 , 0.0 , 500.0 ], [ 318.0 , 0.0 , 1000.0 ]]\n",
    "xdata.append([ 318.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 318.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 318.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 2500.0 ])\n",
    "  \n",
    "xdata.append([ 303.0 , 0.0 , 500.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 1000.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 2500.0 ])\n",
    "  \n",
    "xdata.append([ 288.0 , 0.0 , 500.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 1000.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 2500.0 ])\n",
    "  \n",
    "xdata.append([ 268.0 , 0.0 , 500.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 1000.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 2500.0 ])\n",
    "  '''\n",
    "\n",
    "'''\n",
    "ydata =  [[ 35.1316 , 0.3808 ],[ 40.3764 , 0.38686 ]]\n",
    "ydata.append([ 47.4620 , 0.3930 ])\n",
    "ydata.append([ 57.5639 , 0.39949 ])\n",
    "ydata.append([ 73.1286 , 0.40612 ])\n",
    "ydata.append([ 49.1110 , 0.4023 ])\n",
    "ydata.append([ 56.4428 , 0.40605 ])\n",
    "ydata.append([ 66.3479 , 0.4098 ])\n",
    "ydata.append([ 80.4695 , 0.413 ])\n",
    "ydata.append([ 102.2276 , 0.4175 ])\n",
    "ydata.append([ 63.0904 , 0.41540 ])\n",
    "ydata.append([ 72.5092 , 0.4175 ])\n",
    "ydata.append([ 85.2338, 0.4197 ])\n",
    "ydata.append([ 103.3750 , 0.42192 ])\n",
    "ydata.append([ 131.3266 , 0.4242 ])\n",
    "  \n",
    "ydata.append([ 34.273 , 0.3952 ])\n",
    "ydata.append([ 38.99026 , 0.4012 ])\n",
    "ydata.append([ 45.2133, 0.4073 ])\n",
    "ydata.append([ 53.8000 , 0.4136 ])\n",
    "ydata.append([ 66.4130 , 0.4201 ])\n",
    "ydata.append([ 47.922 , 0.4178 ])\n",
    "ydata.append([ 54.518 , 0.4215 ])\n",
    "ydata.append([ 63.220 , 0.4252 ])\n",
    "ydata.append([ 75.226 , 0.4290 ])\n",
    "ydata.append([ 92.862 , 0.4329 ])\n",
    "ydata.append([ 61.572 , 0.4315 ])\n",
    "ydata.append([ 70.0468 , 0.43373 ])\n",
    "ydata.append([ 81.226 , 0.43597 ])\n",
    "ydata.append([ 96.653 , 0.4382 ])\n",
    "ydata.append([ 119.3124 , 0.44045 ])\n",
    "  \n",
    "ydata.append([ 33.4521 , 0.40913 ])\n",
    "ydata.append([ 37.6911, 0.4150 ])\n",
    "ydata.append([ 43.1602 , 0.4209 ])\n",
    "ydata.append([ 50.4858 , 0.4271 ])\n",
    "ydata.append([ 60.8067 , 0.4334 ])\n",
    "ydata.append([ 46.7865 , 0.4328 ])\n",
    "ydata.append([ 52.7151 , 0.43646 ])\n",
    "ydata.append([ 60.36425 , 0.44016 ])\n",
    "ydata.append([ 70.6099 , 0.443926 ])\n",
    "ydata.append([ 85.0447 , 0.4477 ])\n",
    "ydata.append([ 60.1208 , 0.44721 ])\n",
    "ydata.append([ 67.7391 , 0.44940 ])\n",
    "ydata.append([ 77.56830 , 0.4516 ])\n",
    "ydata.append([ 90.73410 , 0.4538 ])\n",
    "ydata.append([ 109.2828 , 0.4560 ])\n",
    "  \n",
    "ydata.append([ 32.4123 , 0.42694 ])\n",
    "ydata.append([ 36.0807 , 0.4325 ])\n",
    "ydata.append([ 40.6854 , 0.4383 ])\n",
    "ydata.append([ 46.6374 , 0.4442 ])\n",
    "ydata.append([ 54.6293 , 0.4503 ])\n",
    "ydata.append([ 45.3472 , 0.4519 ])\n",
    "ydata.append([ 50.4796 , 0.4555 ])\n",
    "ydata.append([ 56.9219 , 0.4591 ])\n",
    "ydata.append([ 65.2492 , 0.4628 ])\n",
    "ydata.append([ 76.4304 , 0.4665 ])\n",
    "ydata.append([ 58.2822 , 0.4672 ])\n",
    "ydata.append([ 64.8785 , 0.4693 ])\n",
    "ydata.append([ 73.1584 , 0.4715 ])\n",
    "ydata.append([ 83.8610 , 0.4738 ])\n",
    "ydata.append([ 98.2316 , 0.4760 ])\n",
    "  '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152f6d3-1551-425b-a3af-2d8d9bed3b1a",
   "metadata": {},
   "source": [
    "## Code P2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca3926-2923-46f8-9faa-ec381b71b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''>>>>> start CodeP2.4F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for hybrid solar/fossil-fuel gas turbine power system'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#create input data array\n",
    "# meadian values of input variables\n",
    "Tmed = 293.\n",
    "gamed = 0.25\n",
    "qsmed = 1250.\n",
    "#T1(K), gamma, , qsol(kW):\n",
    "xdata = []\n",
    "xdata =  [[ 318.0, 0.0, 500.0], [ 318.0, 0.0, 1000.0]]\n",
    "xdata.append([ 318.0, 0.0, 1500.0])\n",
    "xdata.append([ 318.0, 0.0, 2000.0])\n",
    "xdata.append([ 318.0, 0.0, 2500.0])\n",
    "'''#convert to:\n",
    "xdata =  [[ 318.0/Tmed , 0.0/gamed , 500.0/qsmed ], [ 318.0/Tmed , 0.0/gamed , 1000.0/qsmed ]]\n",
    "xdata.append([ 318.0/Tmed  , 0.0/gamed , 1500.0/qsmed ])\n",
    "xdata.append([ 318.0/Tmed  , 0.0/gamed , 2000.0/qsmed ])\n",
    "xdata.append([ 318.0/Tmed  , 0.0/gamed , 2500.0/qsmed ])'''\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)\n",
    "# meadian values of output variables\n",
    "almed = 60.\n",
    "efmed = 0.4\n",
    "# alpha, effsys\n",
    "ydata = []\n",
    "ydata =  [[ 35.1316, 0.3808], [ 40.3764, 0.38686]]\n",
    "ydata.append([ 47.4620, 0.3930])\n",
    "ydata.append([ 57.5639, 0.39949])\n",
    "ydata.append([ 73.1286, 0.40612])\n",
    "'''#convert to:\n",
    "ydata =  [[ 35.1316/almed , 0.3808/efmed ], [ 40.3764/almed , 0.38686/efmed ]]\n",
    "ydata.append([ 47.4620/almed , 0.3930/efmed ])\n",
    "ydata.append([ 57.5639/almed , 0.39949/efmed ])\n",
    "ydata.append([ 73.1286/almed , 0.40612/efmed ])'''\n",
    "\n",
    "\n",
    "yarray= np.array(ydata)\n",
    "print (ydata)\n",
    "print (yarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d0246-9a34-4bbc-8f57-19f46b465d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "\n",
    "#As seen below, we have created four dense layers. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 1 in our case. \n",
    "#The activation function we have chosen is elu, which stands for exponential linear unit. .\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=0.5)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=K.elu, input_shape=[3],  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(32, activation=K.elu,  kernel_initializer=initializer),\n",
    "    '''in Task 2.2, add 3rd layer here with 16 neurons'''\n",
    "    keras.layers.Dense(2,  kernel_initializer=initializer)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339829d8-2d01-413d-b777-af588178ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean squared error. After the compilation of the model, we’ll use the fit method with ~500 epochs.\n",
    "#Number of epochs can be varied.\n",
    "\n",
    "#from tf.keras import optimizers\n",
    "rms = keras.optimizers.RMSprop(0.020)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691775db-9e04-4e8e-aa45-d438db0dfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,yarray,epochs=800,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261d1c0-7c80-4fac-91f3-2a4be8ab8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "outpt=[]\n",
    "\n",
    "#first point (row [0])comparison of data and prediction\n",
    "# put in a loop to print comparion for all data points\n",
    "\n",
    "test = [[ xarray[0][0] , xarray[0][1] , xarray[0][2] ]]\n",
    "testarray = np.array(test)\n",
    "outpt = model.predict(testarray)\n",
    "print ('row [0] data:  T1= ', xarray[0][0]*Tmed, ', gam= ', xarray[0][1]*gamed, \\\n",
    "    ', qsol= ', xarray[0][2]*qsmed,', alpha= ', yarray[0][0]*almed,\\\n",
    "    ',  predicted alpha = ', outpt[0][0]*almed)\n",
    "\n",
    "#20th point (row [20])comparison of data and prediction\n",
    "test = [[ xarray[20][0] , xarray[20][1] , xarray[20][2] ]]\n",
    "testarray = np.array(test)\n",
    "outpt = model.predict(testarray)\n",
    "print ('row [20] data:  T1= ', xarray[20][0]*Tmed, ', gam= ', xarray[0][1]*gamed, \\\n",
    "    ', qsol= ', xarray[20][2]*qsmed,', alpha= ', yarray[20][0]*almed,\\\n",
    "    ',  predicted alpha = ', outpt[0][0]*almed)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
