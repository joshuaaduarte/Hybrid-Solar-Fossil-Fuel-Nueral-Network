{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cd8036-5b18-46bd-a2aa-11693b7b5c70",
   "metadata": {},
   "source": [
    "# Project Two\n",
    "## Work Done by Joshua Duarte\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72595b5-cd1b-4d38-8ecc-b28245d3f40e",
   "metadata": {},
   "source": [
    "## Code P2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a1e6e22-9369-4fcf-99c5-576ebe93bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909, 0.9558641975308642], [0.9900990099009901, 1.0, 1.0, 0.9969135802469136], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936, 0.9722222222222222], [1.0, 0.896551724137931, 1.009090909090909, 0.9540123456790124], [0.9900990099009901, 1.0, 1.0, 1.0030864197530864], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936, 0.9691358024691358], [1.188118811881188, 0.896551724137931, 1.009090909090909, 1.098456790123457], [1.7821782178217822, 1.0, 1.0, 1.4320987654320987]]\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.23 0.4 0.7 0.72 0.7\n",
      "-0.15 -0.12 0.01\n",
      "E3 =  0.0023304004322608684 icount = 8 rms fractional error = 0.048274221197869865\n",
      "next ws: 1.2296434158887675 0.39957113223001967 0.699579524559478 0.7198734737335699 0.6998677891900275\n",
      "next bs: -0.15042093558581818 -0.12030307362178909 0.009787848464747631\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2296434158887675 0.39957113223001967 0.699579524559478 0.7198734737335699 0.6998677891900275\n",
      "-0.15042093558581818 -0.12030307362178909 0.009787848464747631\n",
      "E3 =  0.0021840849759191035 icount = 8 rms fractional error = 0.04673419493175317\n",
      "next ws: 1.2292964670859656 0.39915326838751664 0.6991697325786222 0.7197502260960085 0.6997389830406646\n",
      "next bs: -0.15083116577443514 -0.12059838745269916 0.009581167826791363\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2292964670859656 0.39915326838751664 0.6991697325786222 0.7197502260960085 0.6997389830406646\n",
      "-0.15083116577443514 -0.12059838745269916 0.009581167826791363\n",
      "E3 =  0.0020469858368763563 icount = 8 rms fractional error = 0.0452436275830791\n",
      "next ws: 1.2289587571591187 0.3987459330220186 0.6987701578062104 0.7196301217470222 0.6996134424991525\n",
      "next bs: -0.1512311573122343 -0.12088628145246659 0.00937971717217059\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2289587571591187 0.3987459330220186 0.6987701578062104 0.7196301217470222 0.6996134424991525\n",
      "-0.1512311573122343 -0.12088628145246659 0.00937971717217059\n",
      "E3 =  0.001918523317958733 icount = 8 rms fractional error = 0.04380095110792382\n",
      "next ws: 1.2286298945367442 0.3983486544742073 0.6983803373102373 0.7195130264113502 0.6994910294723081\n",
      "next bs: -0.151621373583563 -0.12116709283531053 0.00918325775392619\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2286298945367442 0.3983486544742073 0.6983803373102373 0.7195130264113502 0.6994910294723081\n",
      "-0.151621373583563 -0.12116709283531053 0.00918325775392619\n",
      "E3 =  0.0017981542579855589 icount = 8 rms fractional error = 0.04240464901382346\n",
      "next ws: 1.2283094914522146 0.3979609633695991 0.6979998099524185 0.7193988064959883 0.6993716064394926\n",
      "next bs: -0.15200227613306355 -0.12144115718146947 0.008991552202289818\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2283094914522146 0.3979609633695991 0.6979998099524185 0.7193988064959883 0.6993716064394926\n",
      "-0.15200227613306355 -0.12144115718146947 0.008991552202289818\n",
      "E3 =  0.001685369741179922 icount = 8 rms fractional error = 0.04105325494013748\n",
      "next ws: 1.2279971627592152 0.39758239090732356 0.6976281146513041 0.7192873286536903 0.6992550360093652\n",
      "next bs: -0.15237432639858192 -0.1217088096984399 0.00880436363152863\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2279971627592152 0.39758239090732356 0.6976281146513041 0.7192873286536903 0.6992550360093652\n",
      "-0.15237432639858192 -0.1217088096984399 0.00880436363152863\n",
      "E3 =  0.0015796929522765102 icount = 8 rms fractional error = 0.03974535132913672\n",
      "next ws: 1.2276925245874635 0.3972124668937366 0.6972647883820485 0.7191784592800671 0.6991411804073469\n",
      "next bs: -0.15273798770639666 -0.12197038666907267 0.008621454617509594\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2276925245874635 0.3972124668937366 0.6972647883820485 0.7191784592800671 0.6991411804073469\n",
      "-0.15273798770639666 -0.12197038666907267 0.008621454617509594\n",
      "E3 =  0.0014806771685807057 icount = 8 rms fractional error = 0.038479568196391\n",
      "next ws: 1.2273951927986773 0.39685071745632006 0.6969093638460495 0.7190720639280301 0.6990299008770173\n",
      "next bs: -0.15309372759527742 -0.1222262271342624 0.00844258601268088\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2273951927986773 0.39685071745632006 0.6969093638460495 0.7190720639280301 0.6990299008770173\n",
      "-0.15309372759527742 -0.1222262271342624 0.00844258601268088\n",
      "E3 =  0.0013879038809075546 icount = 8 rms fractional error = 0.03725458201225125\n",
      "next ws: 1.2271047811911944 0.3964966623541481 0.6965613667237365 0.7189680066185405 0.6989210569737357\n",
      "next bs: -0.15344202055572648 -0.12247667487218411 0.008267515555266593\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2271047811911944 0.3964966623541481 0.6965613667237365 0.7189680066185405 0.6989210569737357\n",
      "-0.15344202055572648 -0.12247667487218411 0.008267515555266593\n",
      "E3 =  0.0013009810360083622 icount = 8 rms fractional error = 0.036069114710626904\n",
      "next ws: 1.2268208993860124 0.3961498117751603 0.6962203123966728 0.718866149020155 0.6988145057220928\n",
      "next bs: -0.15378335129773843 -0.12272208075536607 0.008095996216005485\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2268208993860124 0.3961498117751603 0.6962203123966728 0.718866149020155 0.6988145057220928\n",
      "-0.15378335129773843 -0.12272208075536607 0.008095996216005485\n",
      "E3 =  0.0012195413937946703 icount = 8 rms fractional error = 0.03492193284734781\n",
      "next ws: 1.2265431503056543 0.3958096624746565 0.6958857019877949 0.7187663494609944 0.6987101005996597\n",
      "next bs: -0.15411821869757727 -0.12296280559352062 0.007927774207215486\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2265431503056543 0.3958096624746565 0.6958857019877949 0.7187663494609944 0.6987101005996597\n",
      "-0.15411821869757727 -0.12296280559352062 0.007927774207215486\n",
      "E3 =  0.0011432409934218496 icount = 8 rms fractional error = 0.03381184693893325\n",
      "next ws: 1.2262711271276836 0.39547569305945945 0.6955570175164394 0.7186684617244492 0.6986076902968047\n",
      "next bs: -0.15444714062597661 -0.12319922360725387 0.007762586553056353\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2262711271276836 0.39547569305945945 0.6955570175164394 0.7186684617244492 0.6986076902968047\n",
      "-0.15444714062597661 -0.12319922360725387 0.007762586553056353\n",
      "E3 =  0.0010717577231419684 icount = 8 rms fractional error = 0.03273771102477949\n",
      "next ws: 1.2260044095531155 0.39514735815142243 0.6952337158908171 0.7185723335625825 0.6985071171844649\n",
      "next bs: -0.15477065993378122 -0.12343172673053196 0.007600158083116251\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2260044095531155 0.39514735815142243 0.6952337158908171 0.7185723335625825 0.6985071171844649\n",
      "-0.15477065993378122 -0.12343172673053196 0.007600158083116251\n",
      "E3 =  0.0010047899898382065 icount = 8 rms fractional error = 0.03169842251340288\n",
      "next ws: 1.2257425591706148 0.3948240810620576 0.6949152213538627 0.7184778048363268 0.6984082153961486\n",
      "next bs: -0.15508935197720675 -0.12366073001586406 0.007440197658453154\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2257425591706148 0.3948240810620576 0.6949152213538627 0.7184778048363268 0.6984082153961486\n",
      "-0.15508935197720675 -0.12366073001586406 0.007440197658453154\n",
      "E3 =  0.0009420554854174719 icount = 8 rms fractional error = 0.030692922399430653\n",
      "next ws: 1.2254851136110987 0.3945052444606191 0.6946009158415857 0.7183847051553083 0.6983108083931158\n",
      "next bs: -0.155403834220835 -0.12388667852792609 0.007282393361372496\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2254851136110987 0.3945052444606191 0.6946009158415857 0.7183847051553083 0.6983108083931158\n",
      "-0.155403834220835 -0.12388667852792609 0.007282393361372496\n",
      "E3 =  0.0008832900489247679 icount = 8 rms fractional error = 0.029720195977226797\n",
      "next ws: 1.2252315790595383 0.3941901782943201 0.6942901264779038 0.7182928508361787 0.6982147058261713\n",
      "next bs: -0.1557147786902611 -0.12411005627891443 0.007126406263502796\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2252315790595383 0.3941901782943201 0.6942901264779038 0.7182928508361787 0.6982147058261713\n",
      "-0.1557147786902611 -0.12411005627891443 0.007126406263502796\n",
      "E3 =  0.0008282466256303556 icount = 8 rms fractional error = 0.028779274237380546\n",
      "next ws: 1.22498142049719 0.3938781438770067 0.693982109069215 0.718202040916281 0.6981196994231454\n",
      "next bs: -0.15602292840490228 -0.12433139801592841 0.006971862207706528\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.22498142049719 0.3938781438770067 0.693982109069215 0.718202040916281 0.6981196994231454\n",
      "-0.15602292840490228 -0.12433139801592841 0.006971862207706528\n",
      "E3 =  0.0007766943279005965 icount = 8 rms fractional error = 0.027869236227435378\n",
      "next ws: 1.2247340487472158 0.3935683125251145 0.6936760258942464 0.7181120518315681 0.6980255574997164\n",
      "next bs: -0.1563291194856929 -0.12455130507506258 0.006818340757682751\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2247340487472158 0.3935683125251145 0.6936760258942464 0.7181120518315681 0.6980255574997164\n",
      "-0.1563291194856929 -0.12455130507506258 0.006818340757682751\n",
      "E3 =  0.0007284176082338459 icount = 8 rms fractional error = 0.02698921281241537\n",
      "next ws: 1.2244888029179597 0.39325973625143096 0.6933709151651328 0.7180226301626892 0.6979320174807712\n",
      "next bs: -0.15663431154607013 -0.12477046717174278 0.006665360012964744\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2244888029179597 0.39325973625143096 0.6933709151651328 0.7180226301626892 0.6979320174807712\n",
      "-0.15663431154607013 -0.12477046717174278 0.006665360012964744\n",
      "E3 =  0.0006832155639682926 icount = 8 rms fractional error = 0.026138392528391882\n",
      "next ws: 1.2242449260510289 0.39295130657812766 0.6930656479963966 0.7179334825140589 0.6978387754716769\n",
      "next bs: -0.15693963050359014 -0.12498969309265984 0.0065123552236950235\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2242449260510289 0.39295130657812766 0.6930656479963966 0.7179334825140589 0.6978387754716769\n",
      "-0.15693963050359014 -0.12498969309265984 0.0065123552236950235\n",
      "E3 =  0.0006409014087981729 icount = 8 rms fractional error = 0.02531603066829737\n",
      "next ws: 1.2240015304403065 0.3926416950202824 0.6927588660428413 0.7178442610091103 0.6977454713228496\n",
      "next bs: -0.15724643060628968 -0.12520995515882657 0.0063586478131583595\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2240015304403065 0.3926416950202824 0.6927588660428413 0.7178442610091103 0.6977454713228496\n",
      "-0.15724643060628968 -0.12520995515882657 0.0063586478131583595\n",
      "E3 =  0.0006013021742781325 icount = 8 rms fractional error = 0.02452146354274419\n",
      "next ws: 1.223757546707513 0.3923292642497843 0.6924488881078442 0.7177545418446213 0.6976516665661795\n",
      "next bs: -0.157556387291277 -0.12543245578630613 0.006203399007968012\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.223757546707513 0.3923292642497843 0.6924488881078442 0.7177545418446213 0.6976516665661795\n",
      "-0.157556387291277 -0.12543245578630613 0.006203399007968012\n",
      "E3 =  0.0005642587574225193 icount = 8 rms fractional error = 0.023754131375879003\n",
      "next ws: 1.2235116462856206 0.39201193030345816 0.6921335647296882 0.7176637923921989 0.6975568106035738\n",
      "next bs: -0.15787164171358425 -0.12565873107975378 0.006045537672391513\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2235116462856206 0.39201193030345816 0.6921335647296882 0.7176637923921989 0.6975568106035738\n",
      "-0.15787164171358425 -0.12565873107975378 0.006045537672391513\n",
      "E3 =  0.0005296265362941086 icount = 8 rms fractional error = 0.023013616323692122\n",
      "next ws: 1.2232621182139656 0.39168693870255994 0.6918100408454192 0.7175713184394906 0.6974601865516628\n",
      "next bs: -0.15819503653242536 -0.12589081983188327 0.005883642582679103\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2232621182139656 0.39168693870255994 0.6918100408454192 0.7175713184394906 0.6974601865516628\n",
      "-0.15819503653242536 -0.12589081983188327 0.005883642582679103\n",
      "E3 =  0.0004972770016679842 icount = 8 rms fractional error = 0.02229970855567364\n",
      "next ws: 1.2230066626509053 0.3913504792089674 0.6914743451478547 0.71747617482628 0.6973608196517687\n",
      "next bs: -0.1585305236507243 -0.1261315557656805 0.005715738853383202\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2230066626509053 0.3913504792089674 0.6914743451478547 0.71747617482628 0.6973608196517687\n",
      "-0.1585305236507243 -0.1261315557656805 0.005715738853383202\n",
      "E3 =  0.00046710137759391895 icount = 8 rms fractional error = 0.021612528255479945\n",
      "next ws: 1.2227420208697226 0.39099697269610445 0.6911206247202265 0.7173770032291261 0.6972573113274504\n",
      "next bs: -0.15888392365226905 -0.12638511184697243 0.0055389187767057695\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2227420208697226 0.39099697269610445 0.6911206247202265 0.7173770032291261 0.6972573113274504\n",
      "-0.15888392365226905 -0.12638511184697243 0.0055389187767057695\n",
      "E3 =  0.00043901855157933847 icount = 8 rms fractional error = 0.020952769544366645\n",
      "next ws: 1.2224632517535439 0.39061761747808227 0.6907395709551339 0.7172717097263503 0.6971475101396006\n",
      "next bs: -0.1592644863081726 -0.12665811874460545 0.005348562721288328\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2224632517535439 0.39061761747808227 0.6907395709551339 0.7172717097263503 0.6971475101396006\n",
      "-0.1592644863081726 -0.12665811874460545 0.005348562721288328\n",
      "E3 =  0.000412993586190917 icount = 8 rms fractional error = 0.02032224363083262\n",
      "next ws: 1.2221621375889002 0.39019700149729497 0.6903147059918033 0.7171567386460879 0.6970277731622578\n",
      "next bs: -0.15968856761859146 -0.12696230027119257 0.005136503327397658\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2221621375889002 0.39019700149729497 0.6903147059918033 0.7171567386460879 0.6970277731622578\n",
      "-0.15968856761859146 -0.12696230027119257 0.005136503327397658\n",
      "E3 =  0.00038908694840807074 icount = 8 rms fractional error = 0.01972528702980189\n",
      "next ws: 1.2218230002379897 0.3897034509803377 0.6898116064575909 0.7170251059677899 0.696890975084889\n",
      "next bs: -0.16019028035095448 -0.12732210693807125 0.004885708087614283\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2218230002379897 0.3897034509803377 0.6898116064575909 0.7170251059677899 0.696890975084889\n",
      "-0.16019028035095448 -0.12732210693807125 0.004885708087614283\n",
      "E3 =  0.0003676179821777909 icount = 8 rms fractional error = 0.019173366480036595\n",
      "next ws: 1.2214080817125266 0.3890502373534693 0.6891331255452056 0.7168591593515998 0.696719237221325\n",
      "next bs: -0.1608656007822733 -0.12780632864189984 0.004548258352275913\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2214080817125266 0.3890502373534693 0.6891331255452056 0.7168591593515998 0.696719237221325\n",
      "-0.1608656007822733 -0.12780632864189984 0.004548258352275913\n",
      "E3 =  0.0003499794936508059 icount = 8 rms fractional error = 0.01870773887060662\n",
      "next ws: 1.2207649255815238 0.387741815062798 0.6876667588637975 0.7165786188771192 0.6964327735054727\n",
      "next bs: -0.16231342389870826 -0.12884421390403722 0.0038251437241163004\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2207649255815238 0.387741815062798 0.6876667588637975 0.7165786188771192 0.6964327735054727\n",
      "-0.16231342389870826 -0.12884421390403722 0.0038251437241163004\n",
      "E3 =  0.00035038728454023867 icount = 8 rms fractional error = 0.018718634686863213\n",
      "next ws: 1.2235301726306473 0.3888601164166848 0.6886465868562739 0.7171189227480413 0.6970410967564209\n",
      "next bs: -0.16132227915810934 -0.12813398077471153 0.0043197733522080594\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2235301726306473 0.3888601164166848 0.6886465868562739 0.7171189227480413 0.6970410967564209\n",
      "-0.16132227915810934 -0.12813398077471153 0.0043197733522080594\n",
      "E3 =  0.00036225435877188775 icount = 8 rms fractional error = 0.01903298081677927\n",
      "next ws: 1.223016269676443 0.38794627272056387 0.6876701279619271 0.7169045476416664 0.6968205685541922\n",
      "next bs: -0.16229136031051242 -0.12882892720677824 0.0038353671290133062\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.223016269676443 0.38794627272056387 0.6876701279619271 0.7169045476416664 0.6968205685541922\n",
      "-0.16229136031051242 -0.12882892720677824 0.0038353671290133062\n",
      "E3 =  0.0003502849771911097 icount = 8 rms fractional error = 0.01871590171995754\n",
      "next ws: 1.2215160969790517 0.4191348743429785 0.6950450855852889 0.7159223943277965 0.6958945362144514\n",
      "next bs: -0.1543217390769469 -0.12311546950145355 0.007816621975647989\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2215160969790517 0.4191348743429785 0.6950450855852889 0.7159223943277965 0.6958945362144514\n",
      "-0.1543217390769469 -0.12311546950145355 0.007816621975647989\n",
      "E3 =  0.001057499071304542 icount = 8 rms fractional error = 0.032519210803839356\n",
      "next ws: 1.2212491686920233 0.41881183750951423 0.6947264906288928 0.7158278349920845 0.6957955702842429\n",
      "next bs: -0.154640500519524 -0.12334367795664271 0.0076578129585639105\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2212491686920233 0.41881183750951423 0.6947264906288928 0.7158278349920845 0.6957955702842429\n",
      "-0.154640500519524 -0.12334367795664271 0.0076578129585639105\n",
      "E3 =  0.000991385383944392 icount = 8 rms fractional error = 0.03148627294464037\n",
      "next ws: 1.2209872673160744 0.4184942426128875 0.6944130729739746 0.7157349325162101 0.6956983328302869\n",
      "next bs: -0.15495406345692503 -0.12356813503525625 0.007501636717545663\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2209872673160744 0.4184942426128875 0.6944130729739746 0.7157349325162101 0.6956983328302869\n",
      "-0.15495406345692503 -0.12356813503525625 0.007501636717545663\n",
      "E3 =  0.0009294485735077919 icount = 8 rms fractional error = 0.030486859029880266\n",
      "next ws: 1.2207299519478851 0.41818152679651677 0.6941042678032672 0.7156435288253981 0.6956026598819596\n",
      "next bs: -0.1552629920339851 -0.12378924600951068 0.007347810181386383\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2207299519478851 0.41818152679651677 0.6941042678032672 0.7156435288253981 0.6956026598819596\n",
      "-0.1552629920339851 -0.12378924600951068 0.007347810181386383\n",
      "E3 =  0.0008714275853880791 icount = 8 rms fractional error = 0.029519952326995367\n",
      "next ws: 1.220476755372125 0.4178730868528934 0.6937994677597612 0.7155534551292113 0.6955083762807922\n",
      "next bs: -0.15556789268511123 -0.12400744618742372 0.007196029557243349\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.220476755372125 0.4178730868528934 0.6937994677597612 0.7155534551292113 0.6955083762807922\n",
      "-0.15556789268511123 -0.12400744618742372 0.007196029557243349\n",
      "E3 =  0.0008170781663386258 icount = 8 rms fractional error = 0.028584579170220885\n",
      "next ws: 1.2202271759452719 0.4175682671765696 0.6934980103249876 0.7154645287468936 0.6954152923938186\n",
      "next bs: -0.15586942668859982 -0.12422320988545893 0.007045964097962541\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2202271759452719 0.4175682671765696 0.6934980103249876 0.7154645287468936 0.6954152923938186\n",
      "-0.15586942668859982 -0.12422320988545893 0.007045964097962541\n",
      "E3 =  0.0007661719072224109 icount = 8 rms fractional error = 0.027679810462183638\n",
      "next ws: 1.2199806669651472 0.41726634385684547 0.6931991611118813 0.7153765489293233 0.6953231997882644\n",
      "next bs: -0.15616832678056625 -0.12443706229890009 0.006897247859340236\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2199806669651472 0.41726634385684547 0.6931991611118813 0.7153765489293233 0.6953231997882644\n",
      "-0.15616832678056625 -0.12443706229890009 0.006897247859340236\n",
      "E3 =  0.0007184953823824019 icount = 8 rms fractional error = 0.026804764173228645\n",
      "next ws: 1.2197366224794832 0.4169665032596349 0.6929020913107796 0.7152892912552022 0.6952318654312735\n",
      "next bs: -0.1564654195773663 -0.12464959551858666 0.00674946858096646\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2197366224794832 0.4169665032596349 0.6929020913107796 0.7152892912552022 0.6952318654312735\n",
      "-0.1564654195773663 -0.12464959551858666 0.00674946858096646\n",
      "E3 =  0.0006738493966144501 icount = 8 rms fractional error = 0.025958609296617762\n",
      "next ws: 1.2194943579352326 0.4166678125537106 0.6926058465643916 0.7152024999537062 0.6951410237459499\n",
      "next bs: -0.1567616565104203 -0.12486149062447446 0.006602152351224322\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2194943579352326 0.4166678125537106 0.6926058465643916 0.7152024999537062 0.6951410237459499\n",
      "-0.1567616565104203 -0.12486149062447446 0.006602152351224322\n",
      "E3 =  0.0006320483601175633 icount = 8 rms fractional error = 0.025140571992648922\n",
      "next ws: 1.219253083155532 0.4163691781365029 0.6923093029268742 0.7151158771301462 0.6950503654678895\n",
      "next bs: -0.15705815758453118 -0.12507354893391753 0.00645474192090424\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.219253083155532 0.4163691781365029 0.6923093029268742 0.7151158771301462 0.6950503654678895\n",
      "-0.15705815758453118 -0.12507354893391753 0.00645474192090424\n",
      "E3 =  0.0005929198280149513 icount = 8 rms fractional error = 0.024349945133715422\n",
      "next ws: 1.2190118635574159 0.41606928530068243 0.6920111027262771 0.7150290672211852 0.6949595215771812\n",
      "next bs: -0.15735627508186753 -0.12528673748951305 0.006306565137423998\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2190118635574159 0.41606928530068243 0.6920111027262771 0.7150290672211852 0.6949595215771812\n",
      "-0.15735627508186753 -0.12528673748951305 0.006306565137423998\n",
      "E3 =  0.0005563042703878434 icount = 8 rms fractional error = 0.023586103332001312\n",
      "next ws: 1.218769562697915 0.415766507720881 0.6917095579590185 0.7149416338316743 0.69486803937283\n",
      "next bs: -0.15765768947658723 -0.1255022575430165 0.0061567874241509465\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.218769562697915 0.415766507720881 0.6917095579590185 0.7149416338316743 0.69486803937283\n",
      "-0.15765768947658723 -0.1255022575430165 0.0061567874241509465\n",
      "E3 =  0.0005220551946774446 icount = 8 rms fractional error = 0.022848527188364783\n",
      "next ws: 1.2185247539030488 0.4154587661986899 0.6914024988260946 0.7148530238717652 0.6947753454618835\n",
      "next bs: -0.15796455974639761 -0.1257216518750891 0.0060043373147741505\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2185247539030488 0.4154587661986899 0.6914024988260946 0.7148530238717652 0.6947753454618835\n",
      "-0.15796455974639761 -0.1257216518750891 0.0060043373147741505\n",
      "E3 =  0.0004900398552935678 icount = 8 rms fractional error = 0.02213684384219141\n",
      "next ws: 1.2182755780469259 0.4151432974189479 0.6910870244085222 0.7147625093982308 0.6946806858029413\n",
      "next bs: -0.15827977066556512 -0.12594698135381338 0.005847783948350739\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2182755780469259 0.4151432974189479 0.6910870244085222 0.7147625093982308 0.6946806858029413\n",
      "-0.15827977066556512 -0.12594698135381338 0.005847783948350739\n",
      "E3 =  0.0004601410299918342 icount = 8 rms fractional error = 0.021450898116205628\n",
      "next ws: 1.218019501512115 0.41481625222841895 0.6907590666276692 0.7146690877311228 0.6945830228696948\n",
      "next bs: -0.15860736500449915 -0.12618113350557453 0.005685122970983079\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.218019501512115 0.41481625222841895 0.6907590666276692 0.7146690877311228 0.6945830228696948\n",
      "-0.15860736500449915 -0.12618113350557453 0.005685122970983079\n",
      "E3 =  0.0004322609194026364 icount = 8 rms fractional error = 0.020790885488661526\n",
      "next ws: 1.2177528739813737 0.41447194270797955 0.6904125661073757 0.7145712969079792 0.6944808459599312\n",
      "next bs: -0.15895335972781133 -0.12642840523884383 0.005513372223018667\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2177528739813737 0.41447194270797955 0.6904125661073757 0.7145712969079792 0.6944808459599312\n",
      "-0.15895335972781133 -0.12642840523884383 0.005513372223018667\n",
      "E3 =  0.0004063297379073133 icount = 8 rms fractional error = 0.020157622327727873\n",
      "next ws: 1.2174700413681983 0.4141012811749702 0.6900377446742861 0.7144668401262182 0.6943717859863812\n",
      "next bs: -0.15932745497762615 -0.12669572296667111 0.00532772518125708\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2174700413681983 0.4141012811749702 0.6900377446742861 0.7144668401262182 0.6943717859863812\n",
      "-0.15932745497762615 -0.12669572296667111 0.00532772518125708\n",
      "E3 =  0.00038232611508363755 icount = 8 rms fractional error = 0.01955316125550131\n",
      "next ws: 1.2171613204813954 0.4136880519118803 0.6896169159124648 0.7143517114218152 0.6942517212865785\n",
      "next bs: -0.15974717227785012 -0.12699559705990845 0.005119501071564824\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2171613204813954 0.4136880519118803 0.6896169159124648 0.7143517114218152 0.6942517212865785\n",
      "-0.15974717227785012 -0.12699559705990845 0.005119501071564824\n",
      "E3 =  0.00036033300422404235 icount = 8 rms fractional error = 0.018982439364424224\n",
      "next ws: 1.2168073897663392 0.41319786525366015 0.6891117779342353 0.7142177034793993 0.6941122398249969\n",
      "next bs: -0.16025037666165484 -0.12735506197267432 0.004869941937134997\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2168073897663392 0.41319786525366015 0.6891117779342353 0.7142177034793993 0.6941122398249969\n",
      "-0.16025037666165484 -0.12735506197267432 0.004869941937134997\n",
      "E3 =  0.00034073309683827933 icount = 8 rms fractional error = 0.018458957089670026\n",
      "next ws: 1.216357733025874 0.4125302702148731 0.6884058318375348 0.714042336036248 0.6939304618676472\n",
      "next bs: -0.16095176378660467 -0.127856005074306 0.004522231198836544\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.216357733025874 0.4125302702148731 0.6884058318375348 0.714042336036248 0.6939304618676472\n",
      "-0.16095176378660467 -0.127856005074306 0.004522231198836544\n",
      "E3 =  0.00032528839043788473 icount = 8 rms fractional error = 0.018035753115350762\n",
      "next ws: 1.2155550173401706 0.4109504557396481 0.6864932972921114 0.7136940904813198 0.6935754652339341\n",
      "next bs: -0.1628238544880018 -0.12919275709200334 0.0035946182537933196\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.216357733025874 0.4125302702148731 0.6884058318375348 0.714042336036248 0.6939304618676472\n",
      "-0.16095176378660467 -0.127856005074306 0.004522231198836544\n",
      "Tdbin, Twbin, qdot, Tdbout, ypredicted:\n",
      "20.0 13.0 310.8 30.97 31.111950636608388\n",
      "20.0 14.5 308.0 32.3 31.696596380009723\n",
      "20.0 15.3 306.0 31.5 31.990227141614145\n",
      "20.2 13.0 310.8 30.91 31.305292042148874\n",
      "20.0 14.5 308.0 32.5 31.696596380009723\n",
      "20.0 15.3 306.0 31.4 31.990227141614145\n",
      "24.0 13.0 310.8 35.59 34.97877874741809\n",
      "36.0 14.5 308.0 46.4 47.163908823248555\n"
     ]
    }
   ],
   "source": [
    "'''#Intro to Neural Network Modeling \n",
    "# Python Neural Network Model of Spray Cooling Test System\n",
    "\n",
    ">>>>> start CodeP2.1F22\n",
    "    V.P. Carey, ME249, Fall 2022'''\n",
    "\n",
    "# version 3 print function\n",
    "from __future__ import print_function\n",
    "\n",
    "# import math, numpy and other usefuk packages\n",
    "import math\n",
    "import numpy \n",
    "\n",
    "%matplotlib inline\n",
    "# importing the required module \n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = [10, 8] # for square canvas\n",
    "\n",
    "\n",
    "#assembling data array\n",
    "#store array where rows are data vectors [x01, x02, x03, y3]\n",
    "xydata = []\n",
    "\n",
    "xydata = [[20./20.2, 13.0/14.5, 310.8/308.0, 30.97/32.4], [20./20.2, 14.5/14.5, 308.0/308.0, 32.3/32.4]]\n",
    "xydata.append([20./20.2, 15.3/14.5, 306.0/308.0, 31.5/32.4])\n",
    "xydata.append([20.2/20.2, 13.0/14.5, 310.8/308.0, 30.91/32.4]) \n",
    "xydata.append([20./20.2, 14.5/14.5, 308.0/308.0, 32.5/32.4]) \n",
    "xydata.append([20./20.2, 15.3/14.5, 306.0/308.0, 31.4/32.4]) \n",
    "xydata.append([24./20.2, 13.0/14.5, 310.8/308.0, 35.59/32.4]) \n",
    "xydata.append([36./20.2, 14.5/14.5, 308.0/308.0, 46.4/32.4]) \n",
    "print (xydata)\n",
    "\n",
    "#set starting values \n",
    "w01n =  1.23 \n",
    "w02n =  0.40 \n",
    "w03n =  0.70\n",
    "b1n =  -0.15\n",
    "w12n =  0.72\n",
    "b2n =  -0.12\n",
    "w23n =  0.7\n",
    "b3n =  0.01\n",
    "\n",
    "#start of batch loop  \n",
    "\n",
    "for k in range (0,200):\n",
    "    icount = 0\n",
    "    #initialize error and derivative parameters\n",
    "    E3ti = 0.\n",
    "    dE3da3 = 0.\n",
    "    dE3dw01ti = 0.\n",
    "    dE3dw02ti = 0.\n",
    "    dE3dw03ti = 0.\n",
    "    dE3db1ti = 0.\n",
    "    dE3dw12ti = 0.\n",
    "    dE3db2ti = 0.\n",
    "    dE3dw23ti = 0.\n",
    "    dE3db3ti = 0.\n",
    " \n",
    "    w01 = w01n \n",
    "    w02 = w02n\n",
    "    w03 = w03n\n",
    "    b1 = b1n \n",
    "    w12 = w12n\n",
    "    b2 = b2n \n",
    "    w23 = w23n \n",
    "    b3 = b3n \n",
    "    \n",
    "    #doing calcuations for each data point \n",
    "    for i in range(0,8):\n",
    "        #compute activation functions and their derivatives\n",
    "        z1 = w01*xydata[i][0]+w02*xydata[i][1]+w03*xydata[i][2]+b1 \n",
    "        sig1 = z1\n",
    "        sigp1 = 1.0\n",
    "        if z1 < 0.0:\n",
    "            sig1 = math.exp(z1) - 1.0\n",
    "            sigp1 = math.exp(z1)\n",
    "        a1 = sig1\n",
    "\n",
    "        z2 = w12*a1+b2 \n",
    "        sig2 = z2\n",
    "        sigp2 = 1.0\n",
    "        if z2 < 0.0:\n",
    "            sig2 = math.exp(z2) - 1.0\n",
    "            sigp2 = math.exp(z2)\n",
    "        a2 = sig2\n",
    "\n",
    "        z3 = w23*a2+b3 \n",
    "        sig3 = z3\n",
    "        sigp3 = 1.0\n",
    "        if z3 < 0.0:\n",
    "            sig3 = math.exp(z3) - 1.0\n",
    "            sigp3 = math.exp(z3)\n",
    "        a3 = sig3\n",
    "        \n",
    "        \n",
    "        #compute derivatives for backpropagation\n",
    "        #add to sum for batch average calculation\n",
    "        E3ti = E3ti +(a3 - xydata[i][3])*(a3 - xydata[i][3])\n",
    "        dE3da3 = 2.*(a3 - xydata[i][3])\n",
    "        \n",
    "        dE3dw01ti = dE3dw01ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][0]\n",
    "        dE3dw02ti = dE3dw02ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][1]\n",
    "        dE3dw03ti = dE3dw03ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][2]\n",
    "        dE3db1ti = dE3db1ti + dE3da3*sigp3*w23*sigp2*w12*sigp1\n",
    "        \n",
    "        dE3dw12ti = dE3dw12ti + dE3da3*sigp3*w23*sigp2*a1\n",
    "        dE3db2ti = dE3db2ti + dE3da3*w23*sigp2\n",
    "        \n",
    "        dE3dw23ti = dE3dw23ti + dE3da3*sigp3*a2\n",
    "        dE3db3ti = dE3db3ti + dE3da3*sigp3\n",
    "        \n",
    "        icount = i + 1\n",
    "        # end  calculations for each data point in batch\n",
    "        \n",
    "    #compute batch averaged values\n",
    "    E3 = E3ti/icount\n",
    "    dE3dw01 = dE3dw01ti/icount\n",
    "    dE3dw02 = dE3dw02ti/icount\n",
    "    dE3dw03 = dE3dw03ti/icount\n",
    "    dE3db1 = dE3db1ti/icount\n",
    "    dE3dw12 = dE3dw12ti/icount\n",
    "    dE3db2 = dE3db2ti/icount\n",
    "    dE3dw23 = dE3dw23ti/icount\n",
    "    dE3db3 = dE3db3ti/icount\n",
    "    \n",
    "    #set gam = learning rate\n",
    "    gam = 0.05\n",
    "    if E3 < 0.07: \n",
    "        gam = 0.008\n",
    "\n",
    "    w01n = w01 + gam*(-E3)/dE3dw01\n",
    "    w02n = w02 + gam*(-E3)/dE3dw02\n",
    "    w03n = w03 + gam*(-E3)/dE3dw03\n",
    "    b1n = b1 + gam*(-E3)/dE3db1\n",
    "    w12n = w12 + gam*(-E3)/dE3dw12\n",
    "    b2n = b2 + gam*(-E3)/dE3db2\n",
    "    \n",
    "    w23n = w23 + gam*(-E3)/dE3dw23\n",
    "    b3n = b3 + gam*(-E3)/dE3db3\n",
    "    \n",
    "    #printing for each iteration\n",
    "    print ('last w01, w02, w03, w12, w23:')\n",
    "    print ('last b1, b2, b3:')\n",
    "    print (w01, w02, w03, w12, w23)\n",
    "    print (b1, b2, b3)\n",
    "    print ('E3 = ', E3, 'icount =', icount, 'rms fractional error =', E3**0.5)\n",
    "    print ('next ws:', w01n, w02n, w03n, w12n, w23n)\n",
    "    print ('next bs:', b1n, b2n, b3n)\n",
    "    \n",
    "    #quit if squared error is below target\n",
    "    if E3 < 0.00034:\n",
    "        break\n",
    "    \n",
    "\n",
    "print ('last w01, w02, w03, w12, w23:')\n",
    "print ('last b1, b2, b3:')\n",
    "print (w01, w02, w03, w12, w23)\n",
    "print (b1, b2, b3)\n",
    "#decomment print statements below if you want to print neuron outputs\n",
    "#print ('z1 =', z1)\n",
    "#print ('a1 =', a1)\n",
    "#print ('z2 =', z2)\n",
    "#print ('a2 =', a2)\n",
    "#print ('z3 =', z3)\n",
    "#print ('a3 =', a3)\n",
    "\n",
    "#print comparison of data and trained network predictions\n",
    "# restore raw data values  \n",
    "xydatar = [[20., 13.0, 310.8, 30.97], [20., 14.5, 308.0, 32.3]]\n",
    "xydatar.append([20., 15.3, 306.0, 31.5])\n",
    "xydatar.append([20.2, 13.0, 310.8, 30.91]) \n",
    "xydatar.append([20., 14.5, 308.0, 32.5]) \n",
    "xydatar.append([20., 15.3, 306.0, 31.4]) \n",
    "xydatar.append([24., 13.0, 310.8, 35.59]) \n",
    "xydatar.append([36., 14.5, 308.0, 46.4])\n",
    "print ('Tdbin, Twbin, qdot, Tdbout, ypredicted:')\n",
    "for i in range(0,8): \n",
    "    z1 = w01*xydata[i][0]+w02*xydata[i][1]+w03*xydata[i][2]+b1 \n",
    "    sig1 = z1\n",
    "    sigp1 = 1.0\n",
    "    if z1 < 0.0:\n",
    "        sig1 = math.exp(z1) - 1.0\n",
    "        sigp1 = math.exp(z1)\n",
    "    a1 = sig1\n",
    "\n",
    "    z2 = w12*a1+b2 \n",
    "    sig2 = z2\n",
    "    sigp2 = 1.0\n",
    "    if z2 < 0.0:\n",
    "        sig2 = math.exp(z2) - 1.0\n",
    "        sigp2 = math.exp(z2)\n",
    "    a2 = sig2\n",
    "\n",
    "    z3 = w23*a2+b3 \n",
    "    sig3 = z3\n",
    "    sigp3 = 1.0\n",
    "    if z3 < 0.0:\n",
    "        sig3 = math.exp(z3) - 1.0\n",
    "        sigp3 = math.exp(z3)\n",
    "    a3 = sig3\n",
    "\n",
    "    print (xydatar[i][0], xydatar[i][1], xydatar[i][2], xydatar[i][3], a3*32.4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e65cfd-2922-4d1a-9cc6-0b42ae5e476d",
   "metadata": {},
   "source": [
    "--- \n",
    "### Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47ff82a9-8bf2-4843-9529-2cca9b031d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E3 =  0.00032528839043788473 icount = 8 rms fractional error = 0.018035753115350762\n",
      "\n",
      "last w01, w02, w03, w12, w23:\n",
      "1.216357733025874 0.4125302702148731 0.6884058318375348 0.714042336036248 0.6939304618676472\n",
      "last b1, b2, b3:\n",
      "-0.16095176378660467 -0.127856005074306 0.004522231198836544\n",
      "\n",
      "Tdbin, Twbin, qdot, Tdbout, ypredicted:\n",
      "20.0 13.0 310.8 30.97 31.111950636608388\n",
      "20.0 14.5 308.0 32.3 31.696596380009723\n",
      "20.0 15.3 306.0 31.5 31.990227141614145\n",
      "20.2 13.0 310.8 30.91 31.305292042148874\n",
      "20.0 14.5 308.0 32.5 31.696596380009723\n",
      "20.0 15.3 306.0 31.4 31.990227141614145\n",
      "24.0 13.0 310.8 35.59 34.97877874741809\n",
      "36.0 14.5 308.0 46.4 47.163908823248555\n",
      "\n",
      "[31.111950636608388, 31.696596380009723, 31.990227141614145, 31.305292042148874, 31.696596380009723, 31.990227141614145, 34.97877874741809, 47.163908823248555]\n"
     ]
    }
   ],
   "source": [
    "print ('E3 = ', E3, 'icount =', icount, 'rms fractional error =', E3**0.5)\n",
    "print('')\n",
    "print ('last w01, w02, w03, w12, w23:')\n",
    "print (w01, w02, w03, w12, w23)\n",
    "print ('last b1, b2, b3:')\n",
    "print (b1, b2, b3)\n",
    "print ('')\n",
    "print ('Tdbin, Twbin, qdot, Tdbout, ypredicted:')\n",
    "fpPredicted = []\n",
    "ydataList = []\n",
    "for i in range(0,8): \n",
    "    z1 = w01*xydata[i][0]+w02*xydata[i][1]+w03*xydata[i][2]+b1 \n",
    "    sig1 = z1\n",
    "    sigp1 = 1.0\n",
    "    if z1 < 0.0:\n",
    "        sig1 = math.exp(z1) - 1.0\n",
    "        sigp1 = math.exp(z1)\n",
    "    a1 = sig1\n",
    "\n",
    "    z2 = w12*a1+b2 \n",
    "    sig2 = z2\n",
    "    sigp2 = 1.0\n",
    "    if z2 < 0.0:\n",
    "        sig2 = math.exp(z2) - 1.0\n",
    "        sigp2 = math.exp(z2)\n",
    "    a2 = sig2\n",
    "\n",
    "    z3 = w23*a2+b3 \n",
    "    sig3 = z3\n",
    "    sigp3 = 1.0\n",
    "    if z3 < 0.0:\n",
    "        sig3 = math.exp(z3) - 1.0\n",
    "        sigp3 = math.exp(z3)\n",
    "    a3 = sig3\n",
    "    fpPredicted.append(a3*32.4)\n",
    "    ydataList.append(xydatar[i][3])\n",
    "\n",
    "    print (xydatar[i][0], xydatar[i][1], xydatar[i][2], xydatar[i][3], a3*32.4)\n",
    "    \n",
    "print('')    \n",
    "print(fpPredicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca4837-82a9-4740-a274-2dff3535b7e3",
   "metadata": {},
   "source": [
    "---\n",
    "## Code P2.2\n",
    "\n",
    "### Task 1.2 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25b419b2-3d08-415c-a078-f9e3a0989716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000000\n",
      "Name: x03, dtype: float64 0    0.955835\n",
      "1    0.996883\n",
      "2    0.972192\n",
      "3    0.953983\n",
      "4    1.003055\n",
      "5    0.969106\n",
      "6    1.098423\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
    "    'y3' : [30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "\n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37722659-e9b5-4c26-9ac4-3f28ff2fed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "Weights and biases of the layers before training the model: \n",
      "\n",
      "dense_one\n",
      "Weights\n",
      "Shape:  (3, 1) \n",
      " [[1.23]\n",
      " [0.4 ]\n",
      " [0.7 ]]\n",
      "Bias\n",
      "Shape:  (1,) \n",
      " [-0.15] \n",
      "\n",
      "dense_two\n",
      "Weights\n",
      "Shape:  (1, 1) \n",
      " [[0.72]]\n",
      "Bias\n",
      "Shape:  (1,) \n",
      " [-0.12] \n",
      "\n",
      "dense_three\n",
      "Weights\n",
      "Shape:  (1, 1) \n",
      " [[0.7]]\n",
      "Bias\n",
      "Shape:  (1,) \n",
      " [0.01] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.dense.Dense at 0x17ad59f9188>,\n",
       " <keras.layers.core.dense.Dense at 0x17ad5d33948>,\n",
       " <keras.layers.core.dense.Dense at 0x17ad5d33d88>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3],  kernel_initializer=initializer, name=\"dense_one\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_two\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_three\")\n",
    "  ])\n",
    "\n",
    "\n",
    "#set starting values to those used in first principles model\n",
    "w01n =  1.23 \n",
    "w02n =  0.40 \n",
    "w03n =  0.70\n",
    "b1n =  -0.15\n",
    "w12n =  0.72\n",
    "b2n =  -0.12\n",
    "w23n =  0.7\n",
    "b3n =  0.01\n",
    "\n",
    "weights0 =  [[ w01n], [w02n], [ w03n]]\n",
    "w0array= np.array(weights0)\n",
    "print(np.shape(w0array))\n",
    "bias0 = [b1n]\n",
    "bias0array= np.array(bias0)\n",
    "L0=[]\n",
    "L0.append(w0array)\n",
    "L0.append(bias0array)\n",
    "model.layers[0].set_weights(L0) \n",
    "\n",
    "weights1 =  [[ w12n]]\n",
    "w1array= np.array(weights1)\n",
    "print(np.shape(w1array))\n",
    "bias1 = [b2n]\n",
    "bias1array= np.array(bias1)\n",
    "L1=[]\n",
    "L1.append(w1array)\n",
    "L1.append(bias1array)\n",
    "model.layers[1].set_weights(L1)\n",
    "\n",
    "weights2 =  [[ w23n]]\n",
    "w2array= np.array(weights2)\n",
    "print(np.shape(w2array))\n",
    "bias2 = [b3n]\n",
    "bias2array= np.array(bias2)\n",
    "L2=[]\n",
    "L2.append(w2array)\n",
    "L2.append(bias2array)\n",
    "model.layers[2].set_weights(L2)\n",
    "\n",
    "\n",
    "print(\"Weights and biases of the layers before training the model: \\n\")\n",
    "for layer in model.layers:\n",
    "  print(layer.name)\n",
    "  print(\"Weights\")\n",
    "  print(\"Shape: \",layer.get_weights()[0].shape,'\\n',layer.get_weights()[0])\n",
    "  print(\"Bias\")\n",
    "  print(\"Shape: \",layer.get_weights()[1].shape,'\\n',layer.get_weights()[1],'\\n')\n",
    "model.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7597c220-6cb9-47fa-af74-48dce695a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "rms = keras.optimizers.RMSprop(0.0035)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "238405ca-6c51-44df-894b-f1945e9e65f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1/1 [==============================] - 0s 465ms/step - loss: 0.0198\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0544\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0165\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0195\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0156\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0151\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0181\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0150\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0147\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0170\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0139\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0216\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0241\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0140\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0139\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0138\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0178\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0181\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0137\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0247\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0267\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0162\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0156\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0174\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0223\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0143\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0247\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0138\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0229\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0214\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0240\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0195\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0164\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0156\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0193\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0195\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0271\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0189\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0146\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0195\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0148\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0272\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0181\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178Restoring model weights from the end of the best epoch: 37.\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0178\n",
      "Epoch 117: early stopping\n",
      "best epoch =  37\n",
      "smallest loss = 0.01374276727437973\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=400,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00ccc57c-253d-4340-8c76-f10cd13b737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2239491]\n",
      " [0.2895847]\n",
      " [0.7234253]]\n",
      "w01 =  1.2239491 w02 =  0.2895847 w03 =  0.7234253\n",
      "[-0.14015044]\n",
      "b1 =  [-0.14015044]\n",
      "[[0.70647335]]\n",
      "w12 =  0.70647335\n",
      "[-0.10168917]\n",
      "b2 =  [-0.10168917]\n",
      "[[0.6799924]]\n",
      "w23 =  0.6799924\n",
      "[0.03707751]\n",
      "b3 =  [0.03707751]\n",
      "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9558346964599856 [[0.9581756]]\n",
      "0.9900990099009901 1.0 1.0 0.9968828122588808 [[0.96940756]]\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9721922162896206 [[0.9748261]]\n",
      "1.0 0.896551724137931 1.009090909090909 0.9539829017622912 [[0.9639972]]\n",
      "0.9900990099009901 1.0 1.0 1.003055461251196 [[0.96940756]]\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.9806477]]\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.0984228881824636 [[1.0746076]]\n",
      "1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.435135]]\n",
      "  \n",
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "20.0 13.0 310.8 30.969044165303533 [[31.044891]]\n",
      "20.0 14.5 308.0 32.29900311718774 [[31.408806]]\n",
      "20.0 15.3 306.0 31.499027807783705 [[31.584368]]\n",
      "20.2 13.0 310.8 30.909046017098234 [[31.233511]]\n",
      "20.0 14.5 308.0 32.498996944538746 [[31.408806]]\n",
      "20.2 15.3 306.0 31.3990308941082 [[31.772987]]\n",
      "23.999999999999996 13.0 310.8 35.58890157711182 [[34.817287]]\n",
      "36.0 14.5 308.0 46.398567945433776 [[46.498375]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray= np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547edcd-73d2-428f-aed4-9833a005da03",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 1.2 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6a37c39-520d-4cbe-913c-f3046f59cba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "20.0 13.0 310.8 30.969044165303533 [[31.044891]]\n",
      "20.0 14.5 308.0 32.29900311718774 [[31.408806]]\n",
      "20.0 15.3 306.0 31.499027807783705 [[31.584368]]\n",
      "20.2 13.0 310.8 30.909046017098234 [[31.233511]]\n",
      "20.0 14.5 308.0 32.498996944538746 [[31.408806]]\n",
      "20.2 15.3 306.0 31.3990308941082 [[31.772987]]\n",
      "23.999999999999996 13.0 310.8 35.58890157711182 [[34.817287]]\n",
      "36.0 14.5 308.0 46.398567945433776 [[46.498375]]\n",
      "\n",
      "smallest loss = 0.01374276727437973\n",
      "\n",
      "[31.044891357421875, 31.40880584716797, 31.584367752075195, 31.233510971069336, 31.40880584716797, 31.772987365722656, 34.81728744506836, 46.498374938964844]\n"
     ]
    }
   ],
   "source": [
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "kerasPredicted = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    kerasPredicted.append(float(a3*32.4))\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)\n",
    "    \n",
    "print('')\n",
    "print('smallest loss =', np.min(loss_hist))\n",
    "print('')\n",
    "#print(kerasPredicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29843a5-5be0-4815-816a-8131100fc388",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 1.2 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "91f23c57-b2f9-4bfb-a584-b240f5a0a4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n",
      "[31.111950636608388, 31.696596380009723, 31.990227141614145, 31.305292042148874, 31.696596380009723, 31.990227141614145, 34.97877874741809, 47.163908823248555]\n",
      "[31.044891357421875, 31.40880584716797, 31.584367752075195, 31.233510971069336, 31.40880584716797, 31.772987365722656, 34.81728744506836, 46.498374938964844]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(ydataList)\n",
    "print(fpPredicted)\n",
    "print(kerasPredicted)\n",
    "print('')\n",
    "\n",
    "plt.scatter(fpPredicted, ydataList)\n",
    "plt.title('First Principle Model')\n",
    "plt.xlabel('Predicted y3 Values')\n",
    "plt.ylabel('Data y3 Values')\n",
    "plt.loglog()\n",
    "#plt.xlim(xmax = 1000, xmin = 10)\n",
    "#plt.ylim(ymax = 1000, ymin = 10)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(kerasPredicted, ydataList)\n",
    "plt.title('Keras Model')\n",
    "plt.xlabel('Predicted y3 Values')\n",
    "plt.ylabel('Data y3 Values')\n",
    "plt.loglog()\n",
    "#plt.xlim(xmax = 1000, xmin = 10)\n",
    "#plt.ylim(ymax = 1000, ymin = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2266733b-a004-4f68-910a-3bc98ef98176",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 1.2 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b01cff69-9922-4abf-a997-18180b03d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000000\n",
      "Name: x03, dtype: float64 0    0.955835\n",
      "1    0.996883\n",
      "2    0.972192\n",
      "3    0.953983\n",
      "4    1.003055\n",
      "5    0.969106\n",
      "6    1.098423\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
    "    'y3' : [30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "\n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dadd2551-09ef-43fe-8ab4-bbf22c648b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "Weights and biases of the layers before training the model: \n",
      "\n",
      "dense_one\n",
      "Weights\n",
      "Shape:  (3, 1) \n",
      " [[1.476]\n",
      " [0.48 ]\n",
      " [0.84 ]]\n",
      "Bias\n",
      "Shape:  (1,) \n",
      " [-0.18] \n",
      "\n",
      "dense_two\n",
      "Weights\n",
      "Shape:  (1, 1) \n",
      " [[0.864]]\n",
      "Bias\n",
      "Shape:  (1,) \n",
      " [-0.144] \n",
      "\n",
      "dense_three\n",
      "Weights\n",
      "Shape:  (1, 1) \n",
      " [[0.84]]\n",
      "Bias\n",
      "Shape:  (1,) \n",
      " [0.012] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.dense.Dense at 0x17ada3a8f48>,\n",
       " <keras.layers.core.dense.Dense at 0x17ada3a8248>,\n",
       " <keras.layers.core.dense.Dense at 0x17adaa792c8>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3],  kernel_initializer=initializer, name=\"dense_one\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_two\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_three\")\n",
    "  ])\n",
    "\n",
    "\n",
    "#set starting values to those used in first principles model\n",
    "w01n =  1.23 * 1.2\n",
    "w02n =  0.40 * 1.2\n",
    "w03n =  0.70 * 1.2\n",
    "b1n =  -0.15 * 1.2\n",
    "w12n =  0.72 * 1.2\n",
    "b2n =  -0.12 * 1.2\n",
    "w23n =  0.7 * 1.2\n",
    "b3n =  0.01 * 1.2\n",
    "\n",
    "weights0 =  [[ w01n], [w02n], [ w03n]]\n",
    "w0array= np.array(weights0)\n",
    "print(np.shape(w0array))\n",
    "bias0 = [b1n]\n",
    "bias0array= np.array(bias0)\n",
    "L0=[]\n",
    "L0.append(w0array)\n",
    "L0.append(bias0array)\n",
    "model.layers[0].set_weights(L0) \n",
    "\n",
    "weights1 =  [[ w12n]]\n",
    "w1array= np.array(weights1)\n",
    "print(np.shape(w1array))\n",
    "bias1 = [b2n]\n",
    "bias1array= np.array(bias1)\n",
    "L1=[]\n",
    "L1.append(w1array)\n",
    "L1.append(bias1array)\n",
    "model.layers[1].set_weights(L1)\n",
    "\n",
    "weights2 =  [[ w23n]]\n",
    "w2array= np.array(weights2)\n",
    "print(np.shape(w2array))\n",
    "bias2 = [b3n]\n",
    "bias2array= np.array(bias2)\n",
    "L2=[]\n",
    "L2.append(w2array)\n",
    "L2.append(bias2array)\n",
    "model.layers[2].set_weights(L2)\n",
    "\n",
    "\n",
    "print(\"Weights and biases of the layers before training the model: \\n\")\n",
    "for layer in model.layers:\n",
    "  print(layer.name)\n",
    "  print(\"Weights\")\n",
    "  print(\"Shape: \",layer.get_weights()[0].shape,'\\n',layer.get_weights()[0])\n",
    "  print(\"Bias\")\n",
    "  print(\"Shape: \",layer.get_weights()[1].shape,'\\n',layer.get_weights()[1],'\\n')\n",
    "model.layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db23bb70-9eab-4603-bb4f-0a9d190228f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "rms = keras.optimizers.RMSprop(0.0035)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd7143a9-db4a-418c-b5eb-1f5702a2ef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 0.0182\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0657\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0175\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0163\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0228\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0154\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0217\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0150\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0210\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0166\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0236\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0151\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0179\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0144\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0144\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0155\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0159\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0179\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0281\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0162\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0141\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0141\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0174\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0233\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0256\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0140\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0140\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0179\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0139\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0139\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0185\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0138\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0144\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0258\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0141\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0165\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0161\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0164\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0224\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0143\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0207\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0248\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0229\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0163\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0156\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0156\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0152\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0179\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0270\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0272\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0196\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0163\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0160\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0158\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0156\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0273\n",
      "Epoch 155/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165Restoring model weights from the end of the best epoch: 75.\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0165\n",
      "Epoch 155: early stopping\n",
      "best epoch =  75\n",
      "smallest loss = 0.013805627822875977\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=400,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "270e672a-4d3b-4170-b2c9-1c46c76bca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2525225 ]\n",
      " [0.31262684]\n",
      " [0.85114336]]\n",
      "w01 =  1.2525225 w02 =  0.31262684 w03 =  0.85114336\n",
      "[-0.18649891]\n",
      "b1 =  [-0.18649891]\n",
      "[[0.70963174]]\n",
      "w12 =  0.70963174\n",
      "[-0.14175826]\n",
      "b2 =  [-0.14175826]\n",
      "[[0.66086984]]\n",
      "w23 =  0.66086984\n",
      "[0.02373085]\n",
      "b3 =  [0.02373085]\n",
      "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9558346964599856 [[0.95840883]]\n",
      "0.9900990099009901 1.0 1.0 0.9968828122588808 [[0.96994704]]\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9721922162896206 [[0.97544414]]\n",
      "1.0 0.896551724137931 1.009090909090909 0.9539829017622912 [[0.96422464]]\n",
      "0.9900990099009901 1.0 1.0 1.003055461251196 [[0.96994704]]\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.98125994]]\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.0984228881824636 [[1.0747257]]\n",
      "1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.435215]]\n",
      "  \n",
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "20.0 13.0 310.8 30.969044165303533 [[31.052448]]\n",
      "20.0 14.5 308.0 32.29900311718774 [[31.426285]]\n",
      "20.0 15.3 306.0 31.499027807783705 [[31.604391]]\n",
      "20.2 13.0 310.8 30.909046017098234 [[31.24088]]\n",
      "20.0 14.5 308.0 32.498996944538746 [[31.426285]]\n",
      "20.2 15.3 306.0 31.3990308941082 [[31.792824]]\n",
      "23.999999999999996 13.0 310.8 35.58890157711182 [[34.821117]]\n",
      "36.0 14.5 308.0 46.398567945433776 [[46.50097]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray= np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7b09f-591c-4e83-b847-09519a5afd29",
   "metadata": {},
   "source": [
    "---\n",
    "## Code P2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12869b-abf9-46c6-a425-b992c0e23707",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''>>>>> start CodeP2.3F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Data arrays for hybrid solar/fossil-fuel gas turbine power system'''\n",
    "\n",
    "'''\n",
    "#create input data array, normalizing input temp\n",
    "#T1(K), gamma, , qsol(kW):\n",
    "xdata = []\n",
    "xdata =  [[ 318.0 , 0.0 , 500.0 ], [ 318.0 , 0.0 , 1000.0 ]]\n",
    "xdata.append([ 318.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 318.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 318.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 318.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 318.0 , 0.5 , 2500.0 ])\n",
    "  \n",
    "xdata.append([ 303.0 , 0.0 , 500.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 1000.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 303.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 303.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 303.0 , 0.5 , 2500.0 ])\n",
    "  \n",
    "xdata.append([ 288.0 , 0.0 , 500.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 1000.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 288.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 288.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 288.0 , 0.5 , 2500.0 ])\n",
    "  \n",
    "xdata.append([ 268.0 , 0.0 , 500.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 1000.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 1500.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 2000.0 ])\n",
    "xdata.append([ 268.0 , 0.0 , 2500.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 500.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 1000.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 1500.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 2000.0 ])\n",
    "xdata.append([ 268.0 , 0.25 , 2500.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 500.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 1000.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 1500.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 2000.0 ])\n",
    "xdata.append([ 268.0 , 0.5 , 2500.0 ])\n",
    "  '''\n",
    "\n",
    "'''\n",
    "ydata =  [[ 35.1316 , 0.3808 ],[ 40.3764 , 0.38686 ]]\n",
    "ydata.append([ 47.4620 , 0.3930 ])\n",
    "ydata.append([ 57.5639 , 0.39949 ])\n",
    "ydata.append([ 73.1286 , 0.40612 ])\n",
    "ydata.append([ 49.1110 , 0.4023 ])\n",
    "ydata.append([ 56.4428 , 0.40605 ])\n",
    "ydata.append([ 66.3479 , 0.4098 ])\n",
    "ydata.append([ 80.4695 , 0.413 ])\n",
    "ydata.append([ 102.2276 , 0.4175 ])\n",
    "ydata.append([ 63.0904 , 0.41540 ])\n",
    "ydata.append([ 72.5092 , 0.4175 ])\n",
    "ydata.append([ 85.2338, 0.4197 ])\n",
    "ydata.append([ 103.3750 , 0.42192 ])\n",
    "ydata.append([ 131.3266 , 0.4242 ])\n",
    "  \n",
    "ydata.append([ 34.273 , 0.3952 ])\n",
    "ydata.append([ 38.99026 , 0.4012 ])\n",
    "ydata.append([ 45.2133, 0.4073 ])\n",
    "ydata.append([ 53.8000 , 0.4136 ])\n",
    "ydata.append([ 66.4130 , 0.4201 ])\n",
    "ydata.append([ 47.922 , 0.4178 ])\n",
    "ydata.append([ 54.518 , 0.4215 ])\n",
    "ydata.append([ 63.220 , 0.4252 ])\n",
    "ydata.append([ 75.226 , 0.4290 ])\n",
    "ydata.append([ 92.862 , 0.4329 ])\n",
    "ydata.append([ 61.572 , 0.4315 ])\n",
    "ydata.append([ 70.0468 , 0.43373 ])\n",
    "ydata.append([ 81.226 , 0.43597 ])\n",
    "ydata.append([ 96.653 , 0.4382 ])\n",
    "ydata.append([ 119.3124 , 0.44045 ])\n",
    "  \n",
    "ydata.append([ 33.4521 , 0.40913 ])\n",
    "ydata.append([ 37.6911, 0.4150 ])\n",
    "ydata.append([ 43.1602 , 0.4209 ])\n",
    "ydata.append([ 50.4858 , 0.4271 ])\n",
    "ydata.append([ 60.8067 , 0.4334 ])\n",
    "ydata.append([ 46.7865 , 0.4328 ])\n",
    "ydata.append([ 52.7151 , 0.43646 ])\n",
    "ydata.append([ 60.36425 , 0.44016 ])\n",
    "ydata.append([ 70.6099 , 0.443926 ])\n",
    "ydata.append([ 85.0447 , 0.4477 ])\n",
    "ydata.append([ 60.1208 , 0.44721 ])\n",
    "ydata.append([ 67.7391 , 0.44940 ])\n",
    "ydata.append([ 77.56830 , 0.4516 ])\n",
    "ydata.append([ 90.73410 , 0.4538 ])\n",
    "ydata.append([ 109.2828 , 0.4560 ])\n",
    "  \n",
    "ydata.append([ 32.4123 , 0.42694 ])\n",
    "ydata.append([ 36.0807 , 0.4325 ])\n",
    "ydata.append([ 40.6854 , 0.4383 ])\n",
    "ydata.append([ 46.6374 , 0.4442 ])\n",
    "ydata.append([ 54.6293 , 0.4503 ])\n",
    "ydata.append([ 45.3472 , 0.4519 ])\n",
    "ydata.append([ 50.4796 , 0.4555 ])\n",
    "ydata.append([ 56.9219 , 0.4591 ])\n",
    "ydata.append([ 65.2492 , 0.4628 ])\n",
    "ydata.append([ 76.4304 , 0.4665 ])\n",
    "ydata.append([ 58.2822 , 0.4672 ])\n",
    "ydata.append([ 64.8785 , 0.4693 ])\n",
    "ydata.append([ 73.1584 , 0.4715 ])\n",
    "ydata.append([ 83.8610 , 0.4738 ])\n",
    "ydata.append([ 98.2316 , 0.4760 ])\n",
    "  '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152f6d3-1551-425b-a3af-2d8d9bed3b1a",
   "metadata": {},
   "source": [
    "## Code P2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca3926-2923-46f8-9faa-ec381b71b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''>>>>> start CodeP2.4F22\n",
    "    V.P. Carey ME249, Fall 2022\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for hybrid solar/fossil-fuel gas turbine power system'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#create input data array\n",
    "# meadian values of input variables\n",
    "Tmed = 293.\n",
    "gamed = 0.25\n",
    "qsmed = 1250.\n",
    "#T1(K), gamma, , qsol(kW):\n",
    "xdata = []\n",
    "xdata =  [[ 318.0, 0.0, 500.0], [ 318.0, 0.0, 1000.0]]\n",
    "xdata.append([ 318.0, 0.0, 1500.0])\n",
    "xdata.append([ 318.0, 0.0, 2000.0])\n",
    "xdata.append([ 318.0, 0.0, 2500.0])\n",
    "'''#convert to:\n",
    "xdata =  [[ 318.0/Tmed , 0.0/gamed , 500.0/qsmed ], [ 318.0/Tmed , 0.0/gamed , 1000.0/qsmed ]]\n",
    "xdata.append([ 318.0/Tmed  , 0.0/gamed , 1500.0/qsmed ])\n",
    "xdata.append([ 318.0/Tmed  , 0.0/gamed , 2000.0/qsmed ])\n",
    "xdata.append([ 318.0/Tmed  , 0.0/gamed , 2500.0/qsmed ])'''\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)\n",
    "# meadian values of output variables\n",
    "almed = 60.\n",
    "efmed = 0.4\n",
    "# alpha, effsys\n",
    "ydata = []\n",
    "ydata =  [[ 35.1316, 0.3808], [ 40.3764, 0.38686]]\n",
    "ydata.append([ 47.4620, 0.3930])\n",
    "ydata.append([ 57.5639, 0.39949])\n",
    "ydata.append([ 73.1286, 0.40612])\n",
    "'''#convert to:\n",
    "ydata =  [[ 35.1316/almed , 0.3808/efmed ], [ 40.3764/almed , 0.38686/efmed ]]\n",
    "ydata.append([ 47.4620/almed , 0.3930/efmed ])\n",
    "ydata.append([ 57.5639/almed , 0.39949/efmed ])\n",
    "ydata.append([ 73.1286/almed , 0.40612/efmed ])'''\n",
    "\n",
    "\n",
    "yarray= np.array(ydata)\n",
    "print (ydata)\n",
    "print (yarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d0246-9a34-4bbc-8f57-19f46b465d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "\n",
    "#As seen below, we have created four dense layers. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 1 in our case. \n",
    "#The activation function we have chosen is elu, which stands for exponential linear unit. .\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=0.5)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=K.elu, input_shape=[3],  kernel_initializer=initializer),\n",
    "    keras.layers.Dense(32, activation=K.elu,  kernel_initializer=initializer),\n",
    "    '''in Task 2.2, add 3rd layer here with 16 neurons'''\n",
    "    keras.layers.Dense(2,  kernel_initializer=initializer)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339829d8-2d01-413d-b777-af588178ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean squared error. After the compilation of the model, we’ll use the fit method with ~500 epochs.\n",
    "#Number of epochs can be varied.\n",
    "\n",
    "#from tf.keras import optimizers\n",
    "rms = keras.optimizers.RMSprop(0.020)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691775db-9e04-4e8e-aa45-d438db0dfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,yarray,epochs=800,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261d1c0-7c80-4fac-91f3-2a4be8ab8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "outpt=[]\n",
    "\n",
    "#first point (row [0])comparison of data and prediction\n",
    "# put in a loop to print comparion for all data points\n",
    "\n",
    "test = [[ xarray[0][0] , xarray[0][1] , xarray[0][2] ]]\n",
    "testarray = np.array(test)\n",
    "outpt = model.predict(testarray)\n",
    "print ('row [0] data:  T1= ', xarray[0][0]*Tmed, ', gam= ', xarray[0][1]*gamed, \\\n",
    "    ', qsol= ', xarray[0][2]*qsmed,', alpha= ', yarray[0][0]*almed,\\\n",
    "    ',  predicted alpha = ', outpt[0][0]*almed)\n",
    "\n",
    "#20th point (row [20])comparison of data and prediction\n",
    "test = [[ xarray[20][0] , xarray[20][1] , xarray[20][2] ]]\n",
    "testarray = np.array(test)\n",
    "outpt = model.predict(testarray)\n",
    "print ('row [20] data:  T1= ', xarray[20][0]*Tmed, ', gam= ', xarray[0][1]*gamed, \\\n",
    "    ', qsol= ', xarray[20][2]*qsmed,', alpha= ', yarray[20][0]*almed,\\\n",
    "    ',  predicted alpha = ', outpt[0][0]*almed)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
